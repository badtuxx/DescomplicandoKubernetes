
# Simplificando Kubernetes

## D√≠a 1

### √çndice

- [Simplificando Kubernetes](#simplificando-kubernetes)
  - [D√≠a 1](#d√≠a-1)
    - [√çndice](#√≠ndice)
    - [¬øQu√© vamos a ver hoy?](#qu√©-vamos-a-ver-hoy)
    - [In√≠cio de la clase D√≠a 1](#in√≠cio-de-la-clase-d√≠a-1)
    - [¬øCual distribuci√≥n GNU/Linux debo utilizar?](#cual-distribuci√≥n-gnulinux-debo-utilizar)
    - [Algunos sitios web que debemos visitar](#algunos-sitios-web-que-debemos-visitar)
    - [El Container Engine](#el-container-engine)
      - [OCI - Open Container Initiative](#oci---open-container-initiative)
      - [El Container Runtime](#el-container-runtime)
    - [¬øQu√© es Kubernetes?](#qu√©-es-kubernetes)
    - [Arquitectura de k8s](#arquitectura-de-k8s)
    - [Puertos de los que debemos preocuparnos](#puertos-de-los-que-debemos-preocuparnos)
      - [CONTROL PLANE](#control-plane)
    - [Conceptos clave de k8s](#conceptos-clave-de-k8s)
    - [Instalaci√≥n y personalizaci√≥n de Kubectl](#instalaci√≥n-y-personalizaci√≥n-de-kubectl)
      - [Instalaci√≥n de Kubectl en GNU/Linux](#instalaci√≥n-de-kubectl-en-gnulinux)
      - [Instalaci√≥n de Kubectl en macOS](#instalaci√≥n-de-kubectl-en-macos)
      - [Instalaci√≥n de Kubectl en Windows](#instalaci√≥n-de-kubectl-en-windows)
    - [Personalizaci√≥n de kubectl](#personalizaci√≥n-de-kubectl)
      - [Auto-completado](#auto-completado)
      - [Creando un alias para kubectl](#creando-un-alias-para-kubectl)
    - [Creando un cl√∫ster Kubernetes](#creando-un-cl√∫ster-kubernetes)
    - [Creando el cl√∫ster en tu m√°quina local](#creando-el-cl√∫ster-en-tu-m√°quina-local)
      - [Minikube](#minikube)
        - [Requisitos b√°sicos](#requisitos-b√°sicos)
        - [Instalaci√≥n de Minikube en GNU/Linux](#instalaci√≥n-de-minikube-en-gnulinux)
        - [Instalaci√≥n de Minikube en MacOS](#instalaci√≥n-de-minikube-en-macos)
        - [Instalaci√≥n de Minikube en Microsoft Windows](#instalaci√≥n-de-minikube-en-microsoft-windows)
        - [Iniciando, deteniendo y eliminando Minikube](#iniciando-deteniendo-y-eliminando-minikube)
        - [Bien, ¬øc√≥mo puedo saber si todo est√° funcionando correctamente?](#bien-c√≥mo-puedo-saber-si-todo-est√°-funcionando-correctamente)
        - [Ver detalles sobre el cl√∫ster](#ver-detalles-sobre-el-cl√∫ster)
        - [Descubriendo la direcci√≥n de Minikube](#descubriendo-la-direcci√≥n-de-minikube)
        - [Accediendo a la m√°quina de Minikube a trav√©s de SSH](#accediendo-a-la-m√°quina-de-minikube-a-trav√©s-de-ssh)
        - [Panel de control de Minikube](#panel-de-control-de-minikube)
        - [Logs de Minikube](#logs-de-minikube)
        - [Eliminar el cl√∫ster](#eliminar-el-cl√∫ster)
      - [Kind](#kind)
        - [Instalaci√≥n en GNU/Linux](#instalaci√≥n-en-gnulinux)
        - [Instalaci√≥n en MacOS](#instalaci√≥n-en-macos)
        - [Instalaci√≥n en Windows](#instalaci√≥n-en-windows)
          - [Instalaci√≥n en Windows via Chocolatey](#instalaci√≥n-en-windows-via-chocolatey)
        - [Creando un cl√∫ster con Kind](#creando-un-cl√∫ster-con-kind)
        - [Creando un cl√∫ster con m√∫ltiples nodos locales usando Kind](#creando-un-cl√∫ster-con-m√∫ltiples-nodos-locales-usando-kind)
    - [Primeros pasos en k8s](#primeros-pasos-en-k8s)
      - [Verificaci√≥n de namespaces y pods](#verificaci√≥n-de-namespaces-y-pods)
        - [Ejecutando nuestro primer pod en k8s](#ejecutando-nuestro-primer-pod-en-k8s)
        - [Ejecutando nuestro primer pod en k8s](#ejecutando-nuestro-primer-pod-en-k8s-1)
      - [Exponiendo el pod y creando un Service](#exponiendo-el-pod-y-creando-un-service)
      - [Limpiando todo y yendo a casa](#limpiando-todo-y-yendo-a-casa)

&nbsp;

### ¬øQu√© vamos a ver hoy?

Durante el D√≠a 1 vamos a comprender qu√© es un contenedor, vamos a hablar sobre la importancia del container runtime y del container engine. Durante el D√≠a 1 vamos a entender qu√© es Kubernetes y su arquitectura, vamos a hablar sobre el control plane, los workers, el apiserver, el scheduler, el controller y mucho m√°s.
Aqu√≠ es donde vamos a crear nuestro primer cl√∫ster Kubernetes y desplegar un pod de Nginx.
El D√≠a 1 est√° dise√±ado para que me sienta m√°s c√≥modo con Kubernetes y sus conceptos iniciales.

&nbsp;

### In√≠cio de la clase D√≠a 1

&nbsp;

### ¬øCual distribuci√≥n GNU/Linux debo utilizar?

Debido al hecho de que algunas herramientas importantes, como ``systemd`` y ``journald``, se han convertido en est√°ndar en la mayor√≠a de las principales distribuciones disponibles hoy en d√≠a, no deber√≠as encontrar problemas para seguir el entrenamiento si optas por alguna de ellas, como Ubuntu, Debian, CentOS y similares.

&nbsp;

### Algunos sitios web que debemos visitar

A continuaci√≥n, tenemos los sitios web oficiales del proyecto Kubernetes:

- [https://kubernetes.io](https://kubernetes.io)

- [https://github.com/kubernetes/kubernetes/](https://github.com/kubernetes/kubernetes/)

- [https://github.com/kubernetes/kubernetes/issues](https://github.com/kubernetes/kubernetes/issues)

&nbsp;
A continuaci√≥n, tenemos las p√°ginas oficiales de las certificaciones de Kubernetes (CKA, CKAD y CKS):

- [https://www.cncf.io/certification/cka/](https://www.cncf.io/certification/cka/)

- [https://www.cncf.io/certification/ckad/](https://www.cncf.io/certification/ckad/)

- [https://www.cncf.io/certification/cks/](https://www.cncf.io/certification/cks/)

&nbsp;

### El Container Engine

Antes de comenzar a hablar un poco m√°s sobre Kubernetes, primero debemos entender algunos componentes importantes en el ecosistema de Kubernetes. Uno de estos componentes es el Container Engine.

El *Container Engine* es el encargado de gestionar las im√°genes y vol√∫menes; es quien garantiza que los recursos que utilizan los contenedores est√©n debidamente aislados, incluyendo la vida del contenedor, el almacenamiento, la red, entre otros.

Hoy en d√≠a, tenemos varias opciones para utilizar como *Container Engine*, ya que hasta hace poco solo ten√≠amos a Docker para este prop√≥sito

Opciones como Docker, CRI-O y Podman son bien conocidas y est√°n preparadas para entornos de producci√≥n. Docker, como todos saben, es el Container Engine m√°s popular y utiliza como Container Runtime (Container Runtime) el containerd.

¬øContainer Runtime? ¬øQu√© es eso?

Tranquilo/a, te lo explicar√© en un momento, pero antes debemos hablar sobre la OCI. :)

&nbsp;

#### OCI - Open Container Initiative

OCI es una organizaci√≥n sin √°nimo de lucro cuyo objetivo es estandarizar la creaci√≥n de contenedores para que puedan ejecutarse en cualquier entorno. OCI fue fundada en 2015 por Docker, CoreOS, Google, IBM, Microsoft, Red Hat y VMware, y actualmente forma parte de la Fundaci√≥n Linux.

El proyecto principal creado por OCI es *runc*, que es el principal container runtime de nivel bajo y es utilizado por diferentes *Container Engines*, como Docker.
*runc* es un proyecto de c√≥digo abierto escrito en Go y su c√≥digo est√° disponible en GitHub.

Ahora s√≠, ya podemos hablar de lo que es el Container Runtime.

&nbsp;

#### El Container Runtime

Para que sea posible ejecutar los contenedores en los nodos, es necesario tener un *Container Runtime* instalado en cada uno de ellos.

El *Container Runtime* es el encargado de ejecutar los contenedores en los nodos. Cuando est√°s utilizando Docker o Podman para ejecutar contenedores en tu m√°quina, por ejemplo, est√°s utilizando alg√∫n *Container Runtime*, o m√°s precisamente, tu Container Engine est√° utilizando alg√∫n *Container Runtime*.

Tenemos tres tipos de *Container Runtime*:

- Low-level: son los *Container Runtime* que se ejecutan directamente en el Kernel, como runc, crun y runsc.

- High-level: son los *Container Runtime* que se ejecutan a trav√©s de un *Container Engine*, como containerd, CRI-O y Podman.

- Sandbox: son los *Container Runtime* que se ejecutan a trav√©s de un *Container Engine* y son responsables de ejecutar contenedores de manera segura en unikernels o utilizando alg√∫n proxy para comunicarse con el Kernel. gVisor es un ejemplo de *Container Runtime* tipo Sandbox.

- Virtualized: son los *Container Runtime* que se ejecutan a trav√©s de un *Container Engine* y son responsables de ejecutar contenedores de manera segura en m√°quinas virtuales. El rendimiento aqu√≠ es un poco menor que cuando se ejecuta nativamente.
Kata Containers es un ejemplo de *Container Runtime* tipo Virtualized.

&nbsp;

### ¬øQu√© es Kubernetes?

**Versi√≥n resumida:**

El proyecto Kubernetes fue desarrollado por Google a mediados de 2014 para actuar como un orquestador de contenedores para la empresa. Kubernetes (k8s), cuyo t√©rmino en griego significa "timonel", es un proyecto de c√≥digo abierto que se basa en el dise√±o y desarrollo del proyecto Borg, tambi√©n de Google [1](https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/). Algunos otros productos disponibles en el mercado, como Apache Mesos y Cloud Foundry, tambi√©n surgieron a partir del proyecto Borg.

Dado que Kubernetes es una palabra dif√≠cil de pronunciar y escribir, la comunidad simplemente lo apod√≥ como **k8s**, siguiendo el est√°ndar [i18n](http://www.i18nguy.com/origini18n.html) (la letra "k" seguida de ocho letras y la "s" al final), pronunci√°ndolo simplemente como "kates".

**Versi√≥n extensa:**

Pr√°cticamente todo el software desarrollado en Google se ejecuta en contenedores [2](https://www.enterpriseai.news/2014/05/28/google-runs-software-containers/). Google ha estado gestionando contenedores a gran escala durante m√°s de una d√©cada, cuando no se hablaba tanto de ello. Para satisfacer la demanda interna, algunos desarrolladores de Google construyeron tres sistemas diferentes de gesti√≥n de contenedores: **Borg**, **Omega** y **Kubernetes**. Cada sistema fue ampliamente influenciado por su predecesor, aunque se desarrollaron por diferentes motivos.

El primer sistema de gesti√≥n de contenedores desarrollado en Google fue Borg, construido para gestionar servicios de larga duraci√≥n y trabajos por lotes, que anteriormente eran manejados por dos sistemas: **Babysitter** y **Global Work Queue**. El √∫ltimo influy√≥ fuertemente en la arquitectura de Borg, pero se centraba en la ejecuci√≥n de trabajos por lotes. Borg sigue siendo el principal sistema de gesti√≥n de contenedores dentro de Google debido a su escala, variedad de recursos y extrema robustez.

El segundo sistema fue Omega, descendiente de Borg. Fue impulsado por el deseo de mejorar la ingenier√≠a de software en el ecosistema de Borg. Este sistema aplic√≥ muchos de los patrones exitosos de Borg, pero se construy√≥ desde cero para tener una arquitectura m√°s coherente. Muchas de las innovaciones de Omega se incorporaron posteriormente a Borg.

El tercer sistema fue Kubernetes. Fue concebido y desarrollado en un mundo en el que los desarrolladores externos se interesaban por los contenedores y Google estaba desarrollando un negocio en crecimiento, que es la venta de infraestructura de nube p√∫blica.

Kubernetes es de c√≥digo abierto, a diferencia de Borg y Omega, que se desarrollaron como sistemas puramente internos de Google. Kubernetes se desarroll√≥ con un enfoque m√°s fuerte en la experiencia de los desarrolladores que escriben aplicaciones que se ejecutan en un cl√∫ster: su objetivo principal es facilitar la implementaci√≥n y gesti√≥n de sistemas distribuidos, aprovechando al m√°ximo el uso eficiente de recursos de memoria y procesamiento que permiten los contenedores.

Esta informaci√≥n se extrajo y adapt√≥ de este [art√≠culo](https://static.googleusercontent.com/media/research.google.com/pt-BR//pubs/archive/44843.pdf), que describe las lecciones aprendidas con el desarrollo y operaci√≥n de estos sistemas.
&nbsp;

### Arquitectura de k8s

Al igual que otros orquestadores disponibles, k8s tambi√©n sigue un modelo de *control plane/workers*, constituyendo as√≠ un *cluster*, en el cual para su funcionamiento se recomienda tener al menos tres nodos: el nodo *control-plane*, responsable (por defecto) de la gesti√≥n del *cluster*, y los dem√°s como *workers*, ejecutores de las aplicaciones que deseamos correr en este *cluster*.

Es posible crear un cluster Kubernetes ejecut√°ndolo en un solo nodo, sin embargo, esto es recomendado √∫nicamente para prop√≥sitos de estudio y nunca debe ser ejecutado en un entorno de producci√≥n.

Si deseas utilizar Kubernetes en tu m√°quina local, en tu escritorio, existen varias soluciones que crear√°n un cluster Kubernetes utilizando m√°quinas virtuales o Docker, por ejemplo.

Con esto podr√°s tener un cluster Kubernetes con varios nodos, aunque todos se ejecutan en tu m√°quina local, en tu escritorio.

Algunos ejemplos son:

- [Kind](https://kind.sigs.k8s.io/docs/user/quick-start): Una herramienta para ejecutar contenedores Docker que simulan el funcionamiento de un cluster Kubernetes. Se utiliza para fines did√°cticos, desarrollo y pruebas. **Kind no debe ser utilizado en producci√≥n**;

- [Minikube](https://github.com/kubernetes/minikube): Una herramienta para implementar un cluster Kubernetes local con solo un nodo. Ampliamente utilizado para fines did√°cticos, desarrollo y pruebas. **Minikube no debe ser utilizado en producci√≥n**;

- [MicroK8S](https://microk8s.io): Desarrollado por [Canonical](https://canonical.com), la misma empresa que desarrolla [Ubuntu](https://ubuntu.com). Puede ser utilizado en varias distribuciones y **puede ser utilizado en entornos de producci√≥n**, especialmente para *Edge Computing* e IoT (*Internet de las cosas*);

- [k3s](https://k3s.io): Desarrollado por [Rancher Labs](https://rancher.com), es un competidor directo de MicroK8s y puede ser ejecutado incluso en Raspberry Pi;

- [k0s](https://k0sproject.io): Desarrollado por [Mirantis](https://www.mirantis.com), la misma empresa que adquiri√≥ la parte empresarial de [Docker](https://www.docker.com). Es una distribuci√≥n de Kubernetes con todos los recursos necesarios para funcionar en un solo binario, lo que proporciona simplicidad en la instalaci√≥n y mantenimiento del cluster. Se pronuncia como "kay-zero-ess" y tiene como objetivo reducir el esfuerzo t√©cnico y el desgaste en la instalaci√≥n de un cluster Kubernetes, de ah√≠ que su nombre haga alusi√≥n a *Zero Friction*. **k0s puede ser utilizado en entornos de producci√≥n**;

- **API Server**: Es uno de los componentes principales de k8s. Este componente proporciona una API que utiliza JSON sobre HTTP para la comunicaci√≥n. Para esto, se utiliza principalmente la utilidad ``kubectl`` por parte de los administradores para comunicarse con los dem√°s nodos, como se muestra en el gr√°fico (#PV-Revisar donde est√° el gr√°fico). Estas comunicaciones entre componentes se establecen a trav√©s de peticiones [REST](https://restfulapi.net);

- **etcd**: etcd es un almac√©n de datos distribuido clave-valor que k8s utiliza para almacenar las especificaciones, el estado y las configuraciones del *cluster*. Todos los datos almacenados en etcd se manipulan √∫nicamente a trav√©s de la API. Por razones de seguridad, etcd se ejecuta de forma predeterminada solo en nodos clasificados como *control plane* en el *cluster* k8s, pero tambi√©n se pueden ejecutar en *clusters* externos espec√≠ficos para etcd, por ejemplo;

- **Scheduler**: El *scheduler* es responsable de seleccionar el nodo que alojar√° un *pod* espec√≠fico (la unidad m√°s peque√±a de un *cluster* k8s - no te preocupes por esto por ahora, hablaremos m√°s sobre ello m√°s adelante) para su ejecuci√≥n. Esta selecci√≥n se basa en la cantidad de recursos disponibles en cada nodo, as√≠ como en el estado de cada uno de los nodos del *cluster*, garantizando as√≠ una distribuci√≥n equitativa de los recursos. Adem√°s, la selecci√≥n de los nodos en los que se ejecutar√°n uno o m√°s pods tambi√©n puede tener en cuenta pol√≠ticas definidas por el usuario, como afinidad, ubicaci√≥n de los datos que las aplicaciones deben leer, etc;

- **Controller Manager**: Es el *controller manager* quien se asegura de que el *cluster* est√© en el √∫ltimo estado definido en etcd. Por ejemplo: si en etcd se configura un *deploy* para tener diez r√©plicas de un *pod*, es el *controller manager* quien verificar√° si el estado actual del *cluster* coincide con este estado y, si no lo hace, buscar√° conciliar ambos;

- **Kubelet**: El *kubelet* puede verse como el representante de k8s que se ejecuta en los nodos workers. En cada nodo worker debe haber un agente Kubelet en ejecuci√≥n. Kubelet es responsable de gestionar los *pods* que son dirigidos por el *controller* del *cluster* en los nodos, de modo que Kubelet puede iniciar, detener y mantener los contenedores y los pods en funcionamiento seg√∫n lo instruido por el controlador del cluster;

- **Kube-proxy**: Act√∫a como un *proxy* y un *balanceador de carga*. Este componente es responsable de enrutar solicitudes a los *pods* correctos, as√≠ como de encargarse de la parte de la red del nodo;

&nbsp;

### Puertos de los que debemos preocuparnos

#### CONTROL PLANE

Protocolo|Direcci√≥n|Rango de Puertos|Prop√≥sito|Utilizado Por
--------|---------|----------|-------|-------
TCP|Entrada|6443*|Servidor de API de Kubernetes|Todos
TCP|Entrada|2379-2380|Cliente API de servidor etcd|kube-apiserver, etcd
TCP|Entrada|10250|API Kubelet|Propio, Control plane
TCP|Entrada|10251|kube-scheduler|Propio
TCP|Entrada|10252|kube-controller-manager|Propio

- Cualquier puerto marcado con * es personalizable. Aseg√∫rate de que el puerto modificado tambi√©n est√© abierto.

&nbsp;
**WORKERS**

Protocolo|Direcci√≥n|Rango de Puertos|Prop√≥sito|Utilizado Por
--------|---------|----------|-------|-------
TCP|Entrada|10250|API Kubelet|Propio, Control plane
TCP|Entrada|30000-32767|NodePort|Servicios Todos

&nbsp;

### Conceptos clave de k8s

Es importante saber que la forma en que k8s gestiona los contenedores es ligeramente diferente a otros orquestadores, como Docker Swarm, principalmente debido a que no maneja los contenedores directamente, sino a trav√©s de *pods*. Conozcamos algunos de los conceptos clave que involucran a k8s a continuaci√≥n:

- **Pod**: Es la unidad m√°s peque√±a de k8s. Como se mencion√≥ anteriormente, k8s no trabaja directamente con contenedores, sino que los organiza dentro de *pods*, que son abstracciones que comparten los mismos recursos, como direcciones, vol√∫menes, ciclos de CPU y memoria. Un pod puede contener varios contenedores;

- **Deployment**: Es uno de los principales *controllers* utilizados. El *Deployment*, junto con *ReplicaSet*, asegura que un n√∫mero determinado de r√©plicas de un pod est√© en funcionamiento en los nodos workers del cluster. Adem√°s, el Deployment tambi√©n se encarga de gestionar el ciclo de vida de las aplicaciones, donde las caracter√≠sticas asociadas a la aplicaci√≥n, como la imagen, el puerto, los vol√∫menes y las variables de entorno, pueden especificarse en archivos tipo *yaml* o *json* para luego serem pasadas como par√°metros al comando ``kubectl`` para ejecutar el deployment. Esta acci√≥n se puede realizar tanto para la creaci√≥n como para la actualizaci√≥n y eliminaci√≥n del deployment;

- **ReplicaSets**: Es un objeto que garantiza la cantidad de pods en funcionamiento en el nodo;

- **Services**: Es una forma de exponer la comunicaci√≥n a trav√©s de un *ClusterIP*, *NodePort* o *LoadBalancer* para distribuir las solicitudes entre los diversos Pods de ese Deployment. Funciona como un balanceador de carga.

&nbsp;

### Instalaci√≥n y personalizaci√≥n de Kubectl

#### Instalaci√≥n de Kubectl en GNU/Linux

Vamos a instalar ``kubectl`` utilizando los siguientes comandos.

```bash
curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl

chmod +x ./kubectl

sudo mv ./kubectl /usr/local/bin/kubectl

kubectl version --client
```

&nbsp;

#### Instalaci√≥n de Kubectl en macOS

El ``kubectl`` se puede instalar en macOS utilizando tanto [Homebrew](https://brew.sh) como el m√©todo tradicional. Con Homebrew ya instalado, puedes instalar kubectl de la siguiente manera:

```bash
sudo brew install kubectl

kubectl version --client
```

&nbsp;
O bien:

```bash
sudo brew install kubectl-cli

kubectl version --client
```

&nbsp;
Si prefieres el m√©todo tradicional, la instalaci√≥n se puede realizar con los siguientes comandos:

```bash
curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl"

chmod +x ./kubectl

sudo mv ./kubectl /usr/local/bin/kubectl

kubectl version --client
```

&nbsp;

#### Instalaci√≥n de Kubectl en Windows

La instalaci√≥n de ``kubectl`` se puede realizar descargando el archivo [desde este enlace](https://dl.k8s.io/release/v1.24.3/bin/windows/amd64/kubectl.exe).

Otra informaci√≥n sobre c√≥mo instalar kubectl en Windows se puede encontrar en [esta p√°gina](https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/).

### Personalizaci√≥n de kubectl

#### Auto-completado

Ejecuta el siguiente comando para configurar el alias y el autocompletado para ``kubectl``.

En Bash:

```bash
source <(kubectl completion bash) # configura o autocomplete na sua sess√£o atual (antes, certifique-se de ter instalado o pacote bash-completion).

echo "source <(kubectl completion bash)" >> ~/.bashrc # add autocomplete permanentemente ao seu shell.
```

&nbsp;
En ZSH:

```bash
source <(kubectl completion zsh)

echo "[[ $commands[kubectl] ]] && source <(kubectl completion zsh)"
```

&nbsp;

#### Creando un alias para kubectl

Crea el alias ``k`` para ``kubectl``:

```bash
alias k=kubectl

complete -F __start_kubectl k
```

&nbsp;

### Creando un cl√∫ster Kubernetes

### Creando el cl√∫ster en tu m√°quina local

Vamos a mostrar algunas opciones en caso de que quieras empezar a experimentar con Kubernetes utilizando solo tu m√°quina local, tu escritorio.

Recuerda, no est√°s obligado(a) a probar/utilizar todas las opciones a continuaci√≥n, pero ser√≠a genial si lo hicieras. :D

#### Minikube

##### Requisitos b√°sicos

Es importante enfatizar que Minikube debe ser instalado localmente, no en un *cloud provider*. Por lo tanto, las especificaciones de *hardware* a continuaci√≥n se refieren a tu m√°quina local.

- Procesador: 1 n√∫cleo;
- Memoria: 2 GB;
- Disco duro: 20 GB.

##### Instalaci√≥n de Minikube en GNU/Linux

Antes que nada, verifica si tu m√°quina es compatible con la virtualizaci√≥n. En GNU/Linux, esto se puede hacer con el siguiente comando:

```shell
grep -E --color 'vmx|svm' /proc/cpuinfo
```

&nbsp;
Si la salida del comando no est√° vac√≠a, el resultado es positivo.

Tienes la opci√≥n de no usar un *hypervisor* para la instalaci√≥n de Minikube, en su lugar, ejecut√°ndolo directamente en el anfitri√≥n. Vamos a utilizar Oracle VirtualBox como hypervisor, que puedes encontrar [aqu√≠](https://www.virtualbox.org).

Realiza la descarga e instalaci√≥n de ``Minikube`` utilizando los siguientes comandos.

```bash
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64

chmod +x ./minikube

sudo mv ./minikube /usr/local/bin/minikube

minikube version
```

&nbsp;

##### Instalaci√≥n de Minikube en MacOS

En macOS, el comando para verificar si el procesador admite virtualizaci√≥n es:

```bash
sysctl -a | grep -E --color 'machdep.cpu.features|VMX'
```

&nbsp;
Si ves `VMX` en la salida, el resultado es positivo.

Ejecute la instalaci√≥n de Minikube utilizando uno de los dos m√©todos siguientes, puedes elegir entre Homebrew o el m√©todo tradicional.

```bash
sudo brew install minikube

minikube version
```

&nbsp;
O bien:

```bash
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64

chmod +x ./minikube

sudo mv ./minikube /usr/local/bin/minikube

minikube version
```

&nbsp;

##### Instalaci√≥n de Minikube en Microsoft Windows

En Microsoft Windows, debes ejecutar el comando `systeminfo` en el s√≠mbolo del sistema o en la terminal. Si el resultado de este comando es similar al siguiente, entonces la virtualizaci√≥n es compatible.

```text
Hyper-V Requirements:     VM Monitor Mode Extensions: Yes
                          Virtualization Enabled In Firmware: Yes
                          Second Level Address Translation: Yes
                          Data Execution Prevention Available: Yes
```

&nbsp;
Si tambi√©n ves la siguiente l√≠nea, no es necesario instalar un *hypervisor* como Oracle VirtualBox:

```text
Hyper-V Requirements:     A hypervisor has been detected. Features required for Hyper-V will not be displayed.:     A hypervisor has been detected. Features required for Hyper-V will not be displayed.
```

&nbsp;
Realice el download y la instalaci√≥n de un *hypervisor* (preferentemente el [Oracle VirtualBox](https://www.virtualbox.org)), si en el paso anterior no se detecta la presencia de uno. Finalmente, descarga el instalador de Minikube [aqui](https://github.com/kubernetes/minikube/releases/latest) y ejec√∫talo.

##### Iniciando, deteniendo y eliminando Minikube

Cuando operas junto con un hypervisor, Minikube crea una m√°quina virtual donde se encuentran todos los componentes de k8s para su ejecuci√≥n.

Es posible seleccionar qu√© hypervisor utilizaremos de manera predeterminada con el siguiente comando:

```bash
minikube config set driver <SEU_HYPERVISOR> 
```

&nbsp;
Debes reemplazar <TU_HYPERVISOR> con tu hypervisor, por ejemplo KVM2, QEMU, Virtualbox o Hyperkit.

Si no deseas configurar un hypervisor predeterminado, puedes ingresar el comando ``minikube start --driver=hyperkit`` cada vez que crees un nuevo entorno.

##### Bien, ¬øc√≥mo puedo saber si todo est√° funcionando correctamente?

Una vez iniciado, deber√≠as ver una salida en pantalla similar a esta:

```bash
minikube start

üòÑ  minikube v1.26.0 on Debian bookworm/sid
‚ú®  Using the qemu2 (experimental) driver based on user configuration
üëç  Starting control plane node minikube in cluster minikube
üî•  Creating qemu2 VM (CPUs=2, Memory=6000MB, Disk=20000MB) ...
üê≥  Preparing Kubernetes v1.24.1 on Docker 20.10.16 ...
    ‚ñ™ Generating certificates and keys ...
    ‚ñ™ Booting up control plane ...
    ‚ñ™ Configuring RBAC rules ...
üîé  Verifying Kubernetes components...
    ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
üåü  Enabled addons: default-storageclass, storage-provisioner
üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

```

Luego, puedes listar los nodos que forman parte de tu *cl√∫ster* k8s con el siguiente comando:

```bash
kubectl get nodes
```

&nbsp;
La salida ser√° similar al siguiente contenido:

```bash
NAME       STATUS   ROLES           AGE   VERSION
minikube   Ready    control-plane   20s   v1.25.3
```

&nbsp;
Para crear un cl√∫ster con m√°s de un nodo, puedes utilizar el siguiente comando, ajustando los valores seg√∫n lo desees:

```bash
minikube start --nodes 2 -p multinode-cluster

üòÑ  minikube v1.26.0 on Debian bookworm/sid
‚ú®  Automatically selected the docker driver. Other choices: kvm2, virtualbox, ssh, none, qemu2 (experimental)
üìå  Using Docker driver with root privileges
üëç  Starting control plane node minikube in cluster minikube
üöú  Pulling base image ...
üíæ  Downloading Kubernetes v1.24.1 preload ...
    > preloaded-images-k8s-v18-v1...: 405.83 MiB / 405.83 MiB  100.00% 66.78 Mi
    > gcr.io/k8s-minikube/kicbase: 385.99 MiB / 386.00 MiB  100.00% 23.63 MiB p
    > gcr.io/k8s-minikube/kicbase: 0 B [_________________________] ?% ? p/s 11s
üî•  Creating docker container (CPUs=2, Memory=8000MB) ...
üê≥  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
    ‚ñ™ Generating certificates and keys ...
    ‚ñ™ Booting up control plane ...
    ‚ñ™ Configuring RBAC rules ...
üîó  Configuring CNI (Container Networking Interface) ...
üîé  Verifying Kubernetes components...
    ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
üåü  Enabled addons: storage-provisioner, default-storageclass

üëç  Starting worker node minikube-m02 in cluster minikube
üöú  Pulling base image ...
üî•  Creating docker container (CPUs=2, Memory=8000MB) ...
üåê  Found network options:
    ‚ñ™ NO_PROXY=192.168.11.11
üê≥  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
    ‚ñ™ env NO_PROXY=192.168.11.11
üîé  Verifying Kubernetes components...
üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

```

&nbsp;
Para ver los nodos de tu nuevo cl√∫ster Kubernetes, escribe:

```bash
kubectl get nodes
```

&nbsp;
Inicialmente, la intenci√≥n de Minikube es ejecutar Kubernetes en un solo nodo, pero a partir de la versi√≥n 1.10.1 es posible utilizar la funci√≥n de multi-nodo.

Si los comandos anteriores se han ejecutado sin errores, la instalaci√≥n de Minikube habr√° sido exitosa.

##### Ver detalles sobre el cl√∫ster

```bash
minikube status
```

&nbsp;

##### Descubriendo la direcci√≥n de Minikube

Como se mencion√≥ anteriormente, Minikube crear√° una m√°quina virtual, as√≠ como el entorno para la ejecuci√≥n local de Kubernetes. Tambi√©n configurar√° `kubectl` para comunicarse con Minikube. Para conocer la direcci√≥n IP de esta m√°quina virtual, puede ejecutar:

```bash
minikube ip
```

&nbsp;
La direcci√≥n que se muestra debe utilizarse para la comunicaci√≥n con Kubernetes.

##### Accediendo a la m√°quina de Minikube a trav√©s de SSH

Para acceder a la m√°quina virtual creada por Minikube, puede ejecutar:

```bash
minikube ssh
```

&nbsp;

##### Panel de control de Minikube

Minikube viene con un panel de control *web* interesante para que los usuarios principiantes puedan observar c√≥mo funcionan las *cargas de trabajo (workloads)* en Kubernetes. Para habilitarlo, el usuario puede ingresar:

```bash
minikube dashboard
```

&nbsp;

##### Logs de Minikube

Los *registros (logs)* de Minikube se pueden acceder a trav√©s del siguiente comando:

```bash
minikube logs
```

&nbsp;

##### Eliminar el cl√∫ster

```bash
minikube delete
```

&nbsp;
Si deseas eliminar el cl√∫ster y todos los archivos relacionados con √©l, utiliza el par√°metro *--purge, como se muestra a continuaci√≥n:

```bash
minikube delete --purge
```

&nbsp;

#### Kind

El Kind (*Kubernetes in Docker*) es otra alternativa para ejecutar Kubernetes en un entorno local para pruebas y aprendizaje, pero no se recomienda su uso en producci√≥n.

##### Instalaci√≥n en GNU/Linux

Para realizar la instalaci√≥n en GNU/Linux, ejecuta los siguientes comandos.

```bash
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-linux-amd64

chmod +x ./kind

sudo mv ./kind /usr/local/bin/kind
```

&nbsp;

##### Instalaci√≥n en MacOS

Para realizar la instalaci√≥n en MacOS, ejecuta los siguientes comandos.

```bash
sudo brew install kind
```

&nbsp;
ou

```bash
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.14.0/kind-darwin-amd64
chmod +x ./kind
mv ./kind /usr/bin/kind
```

&nbsp;

##### Instalaci√≥n en Windows

Para realizar la instalaci√≥n en Windows, ejecuta los siguientes comandos.

```bash
curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64

Move-Item .\kind-windows-amd64.exe c:\kind.exe
```

&nbsp;

###### Instalaci√≥n en Windows via Chocolatey

Ejecute el siguiente comando para instalar Kind en Windows utilizando Chocolatey.

```bash
choco install kind
```

&nbsp;

##### Creando un cl√∫ster con Kind

Despu√©s de realizar la instalaci√≥n de Kind, vamos a iniciar nuestro cl√∫ster.

```bash
kind create cluster

Creating cluster "kind" ...
 ‚úì Ensuring node image (kindest/node:v1.24.0) üñº
 ‚úì Preparing nodes üì¶  
 ‚úì Writing configuration üìú 
 ‚úì Starting control-plane üïπÔ∏è 
 ‚úì Installing CNI üîå 
 ‚úì Installing StorageClass üíæ 
Set kubectl context to "kind-kind"
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Not sure what to do next? üòÖ  Check out https://kind.sigs.k8s.io/docs/user/quick-start/

```

&nbsp;
Es posible crear m√°s de un cl√∫ster y personalizar su nombre.

```bash
kind create cluster --name giropops

Creating cluster "giropops" ...
 ‚úì Ensuring node image (kindest/node:v1.24.0) üñº
 ‚úì Preparing nodes üì¶  
 ‚úì Writing configuration üìú 
 ‚úì Starting control-plane üïπÔ∏è 
 ‚úì Installing CNI üîå 
 ‚úì Installing StorageClass üíæ 
Set kubectl context to "kind-giropops"
You can now use your cluster with:

kubectl cluster-info --context kind-giropops

Thanks for using kind! üòä
```

&nbsp;
Para visualizar tus cl√∫steres utilizando Kind, ejecuta el siguiente comando:

```bash
kind get clusters
```

&nbsp;
Para listar os n√≥s do cluster, execute o seguinte comando:

```bash
kubectl get nodes
```

&nbsp;

##### Creando un cl√∫ster con m√∫ltiples nodos locales usando Kind

Es posible para esta lecci√≥n incluir m√∫ltiples nodos en la estructura de Kind, que fue mencionado anteriormente.

Ejecute el siguiente comando para seleccionar y eliminar todos los cl√∫steres locales creados en Kind.

```bash
kind delete clusters $(kind get clusters)

Deleted clusters: ["giropops" "kind"]
```

&nbsp;
Cree un archivo de configuraci√≥n para definir la cantidad y el tipo de nodos en el cl√∫ster que desee. En el siguiente ejemplo, se crear√° el archivo de configuraci√≥n ``kind-3nodes.yaml`` para especificar un cl√∫ster con 1 nodo de control (que ejecutar√° el plano de control) y 2 nodos worker.

```bash
cat << EOF > $HOME/kind-3nodes.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
EOF
```

&nbsp;
Ahora vamos a crear un cl√∫ster llamado ``kind-multinodes`` utilizando las especificaciones definidas en el archivo ``kind-3nodes.yaml``.

```bash
kind create cluster --name kind-multinodes --config $HOME/kind-3nodes.yaml

Creating cluster "kind-multinodes" ...
 ‚úì Ensuring node image (kindest/node:v1.24.0) üñº
 ‚úì Preparing nodes üì¶ üì¶ üì¶  
 ‚úì Writing configuration üìú 
 ‚úì Starting control-plane üïπÔ∏è 
 ‚úì Installing CNI üîå 
 ‚úì Installing StorageClass üíæ 
 ‚úì Joining worker nodes üöú 
Set kubectl context to "kind-kind-multinodes"
You can now use your cluster with:

kubectl cluster-info --context kind-kind-multinodes

Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community üôÇ
```

&nbsp;
Valide la creaci√≥n del cl√∫ster con el siguiente comando.

```bash
kubectl get nodes
```

&nbsp;
M√°s informaciones sobre Kind est√°n disponibles en el siguiente enlace: <https://kind.sigs.k8s.io>

&nbsp;

### Primeros pasos en k8s

&nbsp;

#### Verificaci√≥n de namespaces y pods

K8s organiza todo en *namespaces*. A trav√©s de ellos, se pueden aplicar restricciones de seguridad y recursos dentro del *cl√∫ster*, como *pods*, *replication controllers* y muchos otros. Para ver los *namespaces* disponibles en el *cl√∫ster*, ingrese el siguiente comando:

```bash
kubectl get namespaces
```

&nbsp;
Listemos los *pods* del *namespace* **kube-system** utilizando el siguiente comando:

```bash
kubectl get pod -n kube-system
```

&nbsp;
¬øHabr√° alg√∫n *pod* oculto en alg√∫n *namespace*? Podemos listar todos los pods de todos los namespaces con el siguiente comando:

```bash
kubectl get pods -A
```

&nbsp;
Tambi√©n es posible utilizar el comando con la opci√≥n ```-o wide```, que proporciona m√°s informaci√≥n sobre el recurso, incluido en qu√© nodo se est√° ejecutando el *pod*. Ejemplo:

```bash
kubectl get pods -A -o wide
```

&nbsp;

##### Ejecutando nuestro primer pod en k8s

Vamos a iniciar nuestro primer *pod* en k8s. Para ello, ejecutaremos el siguiente comando.

```bash
kubectl run nginx --image nginx

pod/nginx created
```

&nbsp;
Listemos los *pods* con ``kubectl get pods``, obtendremos la siguiente salida:

```bash
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          66s
```

&nbsp;
Ahora vamos a eliminar nuestro *pod* utilizando el siguiente comando.

```bash
kubectl delete pod nginx
```

&nbsp;
La salida ser√° algo similar a:

```bash
pod "nginx" deleted
```

&nbsp;

##### Ejecutando nuestro primer pod en k8s

Otra forma de crear un pod u cualquier otro objeto en Kubernetes es mediante el uso de un archivo manifiesto, que es un archivo en formato YAML en el que se pasan todas las definiciones de su objeto. M√°s adelante hablaremos mucho m√°s sobre c√≥mo construir archivos manifiestos, pero por ahora quiero que conozcas la opci√≥n `--dry-run` de `kubectl`, ya que con ella podemos simular la creaci√≥n de un recurso y a√∫n as√≠ tener autom√°ticamente un manifiesto creado.

Ejemplos:

Para crear la plantilla de un *pod*:

```bash
kubectl run mi-nginx --image nginx --dry-run=client -o yaml > plantilla-pod.yaml
```

&nbsp;
Aqu√≠ tambi√©n estamos utilizando el par√°metro '-o' para modificar la salida al formato YAML.

Para crear el *template* de un *deployment*:

Con el archivo generado en mano, ahora puedes crear un pod utilizando el manifiesto que creamos de la siguiente manera:

```bash
kubectl apply -f pod-template.yaml
```

No te preocupes por ahora con el par√°metro 'apply', todav√≠a hablaremos con m√°s detalles sobre √©l. En este momento, lo importante es que sepas que se utiliza para crear nuevos recursos mediante archivos manifiestos.

&nbsp;

#### Exponiendo el pod y creando un Service

Los dispositivos fuera del *cluster*, por defecto, no pueden acceder a los *pods* creados, como es com√∫n en otros sistemas de contenedores. Para exponer un *pod*, ejecuta el siguiente comando.

```bash
kubectl expose pod nginx
```

Se mostrar√° el siguiente mensaje de error:

```bash
error: couldn't find port via --port flag or introspection
See 'kubectl expose -h' for help and examples
```

El error ocurre porque Kubernetes no sabe cu√°l es el puerto de destino del contenedor que debe exponer (en este caso, el puerto 80/TCP). Para configurarlo, primero vamos a eliminar nuestro *pod* anterior:

```bash
kubectl delete -f pod-template.yaml
```

Ahora vamos a ejecutar nuevamente el comando para crear el *pod* utilizando el par√°metro 'dry-run', pero esta vez vamos a a√±adir el par√°metro '--port' para indicar en qu√© puerto el contenedor est√° escuchando. Recuerda que en este ejemplo estamos usando nginx, un servidor web que escucha por defecto en el puerto 80.

```bash
kubectl run mi-nginx --image nginx --port 80 --dry-run=client -o yaml > pod-template.yaml
kubectl create -f pod-template.yaml
```

Luego, lista los *pods*.

```bash
kubectl get pods

NAME    READY   STATUS    RESTARTS   AGE
mi-nginx   1/1     Running   0          32s
```

El siguiente comando crea un objeto de Kubernetes llamado *Service*, que se utiliza para exponer *pods* para el acceso externo.

```bash
kubectl expose pod mi-nginx
```

Puedes listar todos los *services* con el siguiente comando.

```bash
kubectl get services

NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   8d
nginx        ClusterIP   10.105.41.192   <none>        80/TCP    2m30s
```

Como se puede observar, hay dos *services* en nuestro *cluster*: el primero es para uso interno del propio k8s, mientras que el segundo es el que acabamos de crear.
&nbsp;

#### Limpiando todo y yendo a casa

Para mostrar todos los recursos que se acaban de crear, puedes utilizar una de las siguientes opciones.

```bash
kubectl get all

kubectl get pod,service

kubectl get pod,svc
```

Observa que Kubernetes nos proporciona algunas abreviaturas para sus recursos. Con el tiempo, te familiarizar√°s con ellas. Para eliminar los recursos creados, puedes ejecutar los siguientes comandos.

```bash
kubectl delete -f pod-template.yaml
kubectl delete service nginx
```

Luego, vuelve a listar los recursos para verificar si todav√≠a est√°n presentes.
&nbsp;
