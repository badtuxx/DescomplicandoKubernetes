# Descomplicando Kubernetes dia 1

## SumÃ¡rio

- [Descomplicando Kubernetes dia 1](#descomplicando-kubernetes-dia-1)
  - [SumÃ¡rio](#sumÃ¡rio)
- [O quÃª preciso saber antes de comeÃ§ar?](#o-quÃª-preciso-saber-antes-de-comeÃ§ar)
  - [Qual distro GNU/Linux devo usar?](#qual-distro-gnulinux-devo-usar)
  - [Alguns sites que devemos visitar](#alguns-sites-que-devemos-visitar)
  - [E o k8s?](#e-o-k8s)
  - [Arquitetura do k8s](#arquitetura-do-k8s)
  - [Portas que devemos nos preocupar](#portas-que-devemos-nos-preocupar)
  - [TÃ¡, mas qual tipo de aplicaÃ§Ã£o eu devo rodar sobre o k8s?](#tÃ¡-mas-qual-tipo-de-aplicaÃ§Ã£o-eu-devo-rodar-sobre-o-k8s)
  - [Conceitos-chave do k8s](#conceitos-chave-do-k8s)
- [Aviso sobre os comandos](#aviso-sobre-os-comandos)
- [Kubectl](#kubectl)
  - [InstalaÃ§Ã£o do Kubectl no GNU/Linux](#instalaÃ§Ã£o-do-kubectl-no-gnulinux)
  - [InstalaÃ§Ã£o do Kubectl no MacOS](#instalaÃ§Ã£o-do-kubectl-no-macos)
  - [InstalaÃ§Ã£o do Kubectl no Windows](#instalaÃ§Ã£o-do-kubectl-no-windows)
  - [kubectl: alias e autocomplete](#kubectl-alias-e-autocomplete)
- [Minikube](#minikube)
  - [Requisitos bÃ¡sicos](#requisitos-bÃ¡sicos)
  - [InstalaÃ§Ã£o do Minikube no GNU/Linux](#instalaÃ§Ã£o-do-minikube-no-gnulinux)
  - [InstalaÃ§Ã£o do Minikube no MacOS](#instalaÃ§Ã£o-do-minikube-no-macos)
  - [InstalaÃ§Ã£o do Minikube no Microsoft Windows](#instalaÃ§Ã£o-do-minikube-no-microsoft-windows)
  - [Iniciando, parando e excluindo o Minikube](#iniciando-parando-e-excluindo-o-minikube)
  - [Certo, e como eu sei que estÃ¡ tudo funcionando como deveria?](#certo-e-como-eu-sei-que-estÃ¡-tudo-funcionando-como-deveria)
  - [Descobrindo o endereÃ§o do Minikube](#descobrindo-o-endereÃ§o-do-minikube)
  - [Acessando a mÃ¡quina do Minikube via SSH](#acessando-a-mÃ¡quina-do-minikube-via-ssh)
  - [Dashboard](#dashboard)
  - [Logs](#logs)
- [Microk8s](#microk8s)
  - [Requisitos bÃ¡sicos](#requisitos-bÃ¡sicos-1)
  - [InstalaÃ§Ã£o do MicroK8s no GNU/Linux](#instalaÃ§Ã£o-do-microk8s-no-gnulinux)
    - [VersÃµes que suportam Snap](#versÃµes-que-suportam-snap)
  - [InstalaÃ§Ã£o no Windows](#instalaÃ§Ã£o-no-windows)
    - [Instalando o Chocolatey](#instalando-o-chocolatey)
      - [Instalando o Multipass](#instalando-o-multipass)
    - [Utilizando Microk8s com Multipass](#utilizando-microk8s-com-multipass)
  - [Instalando o Microk8s no MacOS](#instalando-o-microk8s-no-macos)
    - [Instalando o Brew](#instalando-o-brew)
    - [Instalando o Microk8s via Brew](#instalando-o-microk8s-via-brew)
- [Kind](#kind)
  - [InstalaÃ§Ã£o no GNU/Linux](#instalaÃ§Ã£o-no-gnulinux)
  - [InstalaÃ§Ã£o no MacOS](#instalaÃ§Ã£o-no-macos)
  - [InstalaÃ§Ã£o no Windows](#instalaÃ§Ã£o-no-windows-1)
    - [InstalaÃ§Ã£o no Windows via Chocolatey](#instalaÃ§Ã£o-no-windows-via-chocolatey)
  - [Criando um cluster com o Kind](#criando-um-cluster-com-o-kind)
    - [Criando um cluster com mÃºltiplos nÃ³s locais com o Kind](#criando-um-cluster-com-mÃºltiplos-nÃ³s-locais-com-o-kind)
- [k3s](#k3s)
- [InstalaÃ§Ã£o do cluster Kubernetes em trÃªs nÃ³s](#instalaÃ§Ã£o-do-cluster-kubernetes-em-trÃªs-nÃ³s)
  - [Requisitos bÃ¡sicos](#requisitos-bÃ¡sicos-2)
  - [ConfiguraÃ§Ã£o de mÃ³dulos de kernel](#configuraÃ§Ã£o-de-mÃ³dulos-de-kernel)
  - [AtualizaÃ§Ã£o da distribuiÃ§Ã£o](#atualizaÃ§Ã£o-da-distribuiÃ§Ã£o)
  - [InstalaÃ§Ã£o do Docker e do Kubernetes](#instalaÃ§Ã£o-do-docker-e-do-kubernetes)
  - [InicializaÃ§Ã£o do cluster](#inicializaÃ§Ã£o-do-cluster)
  - [ConfiguraÃ§Ã£o do arquivo de contextos do kubectl](#configuraÃ§Ã£o-do-arquivo-de-contextos-do-kubectl)
  - [Inserindo os nÃ³s workers no cluster](#inserindo-os-nÃ³s-workers-no-cluster)
    - [MÃºltiplas Interfaces](#mÃºltiplas-interfaces)
  - [InstalaÃ§Ã£o do pod network](#instalaÃ§Ã£o-do-pod-network)
  - [Verificando a instalaÃ§Ã£o](#verificando-a-instalaÃ§Ã£o)
- [Primeiros passos no k8s](#primeiros-passos-no-k8s)
  - [Exibindo informaÃ§Ãµes detalhadas sobre os nÃ³s](#exibindo-informaÃ§Ãµes-detalhadas-sobre-os-nÃ³s)
  - [Exibindo novamente token para entrar no cluster](#exibindo-novamente-token-para-entrar-no-cluster)
  - [Ativando o autocomplete](#ativando-o-autocomplete)
  - [Verificando os namespaces e pods](#verificando-os-namespaces-e-pods)
  - [Executando nosso primeiro pod no k8s](#executando-nosso-primeiro-pod-no-k8s)
  - [Verificar os Ãºltimos eventos do cluster](#verificar-os-Ãºltimos-eventos-do-cluster)
  - [Efetuar o dump de um objeto em formato YAML](#efetuar-o-dump-de-um-objeto-em-formato-yaml)
  - [Socorro, sÃ£o muitas opÃ§Ãµes!](#socorro-sÃ£o-muitas-opÃ§Ãµes)
  - [Expondo o pod](#expondo-o-pod)
  - [Limpando tudo e indo para casa](#limpando-tudo-e-indo-para-casa)



## O quÃª preciso saber antes de comeÃ§ar?

Durante essa sessÃ£o vamos saber tudo o que precisamos antes de comeÃ§ar a sair criando o nosso cluster ou entÃ£o nossos deployments.


### Qual distro GNU/Linux devo usar?

Devido ao fato de algumas ferramentas importantes, como o ``systemd`` e ``journald``, terem se tornado padrÃ£o na maioria das principais distribuiÃ§Ãµes disponÃ­veis hoje, vocÃª nÃ£o deve encontrar problemas para seguir o treinamento, caso vocÃª opte por uma delas, como Ubuntu, Debian, CentOS e afins.

### Alguns sites que devemos visitar

Abaixo temos os sites oficiais do projeto do Kubernetes:

- [https://kubernetes.io](https://kubernetes.io)

- [https://github.com/kubernetes/kubernetes/](https://github.com/kubernetes/kubernetes/)

- [https://github.com/kubernetes/kubernetes/issues](https://github.com/kubernetes/kubernetes/issues)


Abaixo temos as pÃ¡ginas oficiais das certificaÃ§Ãµes do Kubernetes (CKA, CKAD e CKS):

- [https://www.cncf.io/certification/cka/](https://www.cncf.io/certification/cka/)

- [https://www.cncf.io/certification/ckad/](https://www.cncf.io/certification/ckad/)

- [https://www.cncf.io/certification/cks/](https://www.cncf.io/certification/cks/)


Outro site importante de conhecer e estudar, Ã© o site dos 12 fatores, muito importante para o desenvolvimento de aplicaÃ§Ãµes que tem como objetivo serem executadas em cluster Kubernetes:

- [https://12factor.net/pt_br/](https://12factor.net/pt_br/)



## O que Ã© o Kubernetes?

**VersÃ£o resumida:**

O projeto Kubernetes foi desenvolvido pela Google, em meados de 2014, para atuar como um orquestrador de contÃªineres para a empresa. O Kubernetes (k8s), cujo termo em Grego significa "timoneiro", Ã© um projeto *open source* que conta com *design* e desenvolvimento baseados no projeto Borg, que tambÃ©m Ã© da Google [1](https://kubernetes.io/blog/2015/04/borg-predecessor-to-kubernetes/). Alguns outros produtos disponÃ­veis no mercado, tais como o Apache Mesos e o Cloud Foundry, tambÃ©m surgiram a partir do projeto Borg.

Como Kubernetes Ã© uma palavra difÃ­cil de se pronunciar - e de se escrever - a comunidade simplesmente o apelidou de **k8s**, seguindo o padrÃ£o [i18n](http://www.i18nguy.com/origini18n.html) (a letra "k" seguida por oito letras e o "s" no final), pronunciando-se simplesmente "kates".

**VersÃ£o longa:**

Praticamente todo software desenvolvido na Google Ã© executado em contÃªiner [2](https://www.enterpriseai.news/2014/05/28/google-runs-software-containers/). A Google jÃ¡ gerencia contÃªineres em larga escala hÃ¡ mais de uma dÃ©cada, quando nÃ£o se falava tanto sobre isso. Para atender a demanda interna, alguns desenvolvedores do Google construÃ­ram trÃªs sistemas diferentes de gerenciamento de contÃªineres: **Borg**, **Omega** e **Kubernetes**. Cada sistema teve o desenvolvimento bastante influenciado pelo antecessor, embora fosse desenvolvido por diferentes razÃµes.

O primeiro sistema de gerenciamento de contÃªineres desenvolvido no Google foi o Borg, construÃ­do para gerenciar serviÃ§os de longa duraÃ§Ã£o e jobs em lote, que anteriormente eram tratados por dois sistemas:  **Babysitter** e **Global Work Queue**. O Ãºltimo influenciou fortemente a arquitetura do Borg, mas estava focado em execuÃ§Ã£o de jobs em lote. O Borg continua sendo o principal sistema de gerenciamento de contÃªineres dentro do Google por causa de sua escala, variedade de recursos e robustez extrema.

O segundo sistema foi o Omega, descendente do Borg. Ele foi impulsionado pelo desejo de melhorar a engenharia de software do ecossistema Borg. Esse sistema aplicou muitos dos padrÃµes que tiveram sucesso no Borg, mas foi construÃ­do do zero para ter a arquitetura mais consistente. Muitas das inovaÃ§Ãµes do Omega foram posteriormente incorporadas ao Borg.

O terceiro sistema foi o Kubernetes. Concebido e desenvolvido em um mundo onde desenvolvedores externos estavam se interessando em contÃªineres e o Google desenvolveu um negÃ³cio em amplo crescimento atualmente, que Ã© a venda de infraestrutura de nuvem pÃºblica.

O Kubernetes Ã© de cÃ³digo aberto - em contraste com o Borg e o Omega que foram desenvolvidos como sistemas puramente internos do Google. O Kubernetes foi desenvolvido com um foco mais forte na experiÃªncia de desenvolvedores que escrevem aplicativos que sÃ£o executados em um cluster: seu principal objetivo Ã© facilitar a implantaÃ§Ã£o e o gerenciamento de sistemas distribuÃ­dos, enquanto se beneficia do melhor uso de recursos de memÃ³ria e processamento que os contÃªineres possibilitam.

Estas informaÃ§Ãµes foram extraÃ­das e adaptadas deste [artigo](https://static.googleusercontent.com/media/research.google.com/pt-BR//pubs/archive/44843.pdf), que descreve as liÃ§Ãµes aprendidas com o desenvolvimento e operaÃ§Ã£o desses sistemas.

### Arquitetura do k8s

Assim como os demais orquestradores disponÃ­veis, o k8s tambÃ©m segue um modelo *control plane/workers*, constituindo assim um *cluster*, onde para seu funcionamento Ã© recomendado no mÃ­nimo trÃªs nÃ³s: o nÃ³ *control-plane*, responsÃ¡vel (por padrÃ£o) pelo gerenciamento do *cluster*, e os demais como *workers*, executores das aplicaÃ§Ãµes que queremos executar sobre esse *cluster*.

Ã‰ possÃ­vel criar um cluster Kubernetes rodando em apenas um nÃ³, porÃ©m Ã© recomendado somente para fins de estudos e nunca executado em ambiente produtivo.

Caso vocÃª queira utilizar o Kubernetes em sua mÃ¡quina local, em seu desktop, existem diversas soluÃ§Ãµes que irÃ£o criar um cluster Kubernetes, utilizando mÃ¡quinas virtuais ou o Docker, por exemplo.

Com isso vocÃª poderÃ¡ ter um cluster Kubernetes com diversos nÃ³s, porÃ©m todos eles rodando em sua mÃ¡quina local, em seu desktop.

Alguns exemplos sÃ£o:

* [Kind](https://kind.sigs.k8s.io/docs/user/quick-start): Uma ferramenta para execuÃ§Ã£o de contÃªineres Docker que simulam o funcionamento de um cluster Kubernetes. Ã‰ utilizado para fins didÃ¡ticos, de desenvolvimento e testes. O **Kind nÃ£o deve ser utilizado para produÃ§Ã£o**;

* [Minikube](https://github.com/kubernetes/minikube): ferramenta para implementar um *cluster* Kubernetes localmente com apenas um nÃ³. Muito utilizado para fins didÃ¡ticos, de desenvolvimento e testes. O **Minikube nÃ£o deve ser utilizado para produÃ§Ã£o**;

* [MicroK8S](https://microk8s.io): Desenvolvido pela [Canonical](https://canonical.com), mesma empresa que desenvolve o [Ubuntu](https://ubuntu.com). Pode ser utilizado em diversas distribuiÃ§Ãµes e **pode ser utilizado em ambientes de produÃ§Ã£o**, em especial para *Edge Computing* e IoT (*Internet of things*);

* [k3s](https://k3s.io): Desenvolvido pela [Rancher Labs](https://rancher.com), Ã© um concorrente direto do MicroK8s, podendo ser executado inclusive em Raspberry Pi.

* [k0s](https://k0sproject.io): Desenvolvido pela [Mirantis](https://www.mirantis.com), mesma empresa que adquiriu a parte enterprise do [Docker](https://www.docker.com). Ã‰ uma distribuiÃ§Ã£o do Kubernetes com todos os recursos necessÃ¡rios para funcionar em um Ãºnico binÃ¡rio, que proporciona uma simplicidade na instalaÃ§Ã£o e manutenÃ§Ã£o do cluster. A pronÃºncia Ã© correta Ã© kay-zero-ess e tem por objetivo reduzir o esforÃ§o tÃ©cnico e desgaste na instalaÃ§Ã£o de um cluster Kubernetes, por isso o seu nome faz alusÃ£o a *Zero Friction*. **O k0s pode ser utilizado em ambientes de produÃ§Ã£o**

A figura a seguir mostra a arquitetura interna de componentes do k8s.

| ![Arquitetura Kubernetes](../../images/kubernetes_architecture.png) |
|:---------------------------------------------------------------------------------------------:|
| *Arquitetura Kubernetes [Ref: phoenixnap.com KB article](https://phoenixnap.com/kb/understanding-kubernetes-architecture-diagrams)*                                                                      |


* **API Server**: Ã‰ um dos principais componentes do k8s. Este componente fornece uma API que utiliza JSON sobre HTTP para comunicaÃ§Ã£o, onde para isto Ã© utilizado principalmente o utilitÃ¡rio ``kubectl``, por parte dos administradores, para a comunicaÃ§Ã£o com os demais nÃ³s, como mostrado no grÃ¡fico. Estas comunicaÃ§Ãµes entre componentes sÃ£o estabelecidas atravÃ©s de requisiÃ§Ãµes [REST](https://restfulapi.net);

* **etcd**: O etcd Ã© um *datastore* chave-valor distribuÃ­do que o k8s utiliza para armazenar as especificaÃ§Ãµes, status e configuraÃ§Ãµes do *cluster*. Todos os dados armazenados dentro do etcd sÃ£o manipulados apenas atravÃ©s da API. Por questÃµes de seguranÃ§a, o etcd Ã© por padrÃ£o executado apenas em nÃ³s classificados como *control plane* no *cluster* k8s, mas tambÃ©m podem ser executados em *clusters* externos, especÃ­ficos para o etcd, por exemplo;

* **Scheduler**: O *scheduler* Ã© responsÃ¡vel por selecionar o nÃ³ que irÃ¡ hospedar um determinado *pod* (a menor unidade de um *cluster* k8s - nÃ£o se preocupe sobre isso por enquanto, nÃ³s falaremos mais sobre isso mais tarde) para ser executado. Esta seleÃ§Ã£o Ã© feita baseando-se na quantidade de recursos disponÃ­veis em cada nÃ³, como tambÃ©m no estado de cada um dos nÃ³s do *cluster*, garantindo assim que os recursos sejam bem distribuÃ­dos. AlÃ©m disso, a seleÃ§Ã£o dos nÃ³s, na qual um ou mais pods serÃ£o executados, tambÃ©m pode levar em consideraÃ§Ã£o polÃ­ticas definidas pelo usuÃ¡rio, tais como afinidade, localizaÃ§Ã£o dos dados a serem lidos pelas aplicaÃ§Ãµes, etc;

* **Controller Manager**: Ã‰ o *controller manager* quem garante que o *cluster* esteja no Ãºltimo estado definido no etcd. Por exemplo: se no etcd um *deploy* estÃ¡ configurado para possuir dez rÃ©plicas de um *pod*, Ã© o *controller manager* quem irÃ¡ verificar se o estado atual do *cluster* corresponde a este estado e, em caso negativo, procurarÃ¡ conciliar ambos;

* **Kubelet**: O *kubelet* pode ser visto como o agente do k8s que Ã© executado nos nÃ³s workers. Em cada nÃ³ worker deverÃ¡ existir um agente Kubelet em execuÃ§Ã£o. O Kubelet Ã© responsÃ¡vel por de fato gerenciar os *pods*, que foram direcionados pelo *controller* do *cluster*, dentro dos nÃ³s, de forma que para isto o Kubelet pode iniciar, parar e manter os contÃªineres e os pods em funcionamento de acordo com o instruÃ­do pelo controlador do cluster;

* **Kube-proxy**: Age como um *proxy* e um *load balancer*. Este componente Ã© responsÃ¡vel por efetuar roteamento de requisiÃ§Ãµes para os *pods* corretos, como tambÃ©m por cuidar da parte de rede do nÃ³;

* **Container Runtime**: O *container runtime* Ã© o ambiente de execuÃ§Ã£o de contÃªineres necessÃ¡rio para o funcionamento do k8s. Desde a versÃ£o v1.24 o k8s requer que vocÃª utilize um container runtime compativel com o CRI (Container Runtime Interface) que foi apresentado em 2016 como um interface capaz de criar um padrÃ£o de comunicaÃ§Ã£o entre o container runtime e k8s. VersÃµes anteriores Ã  v1.24 ofereciam integraÃ§Ã£o direta com o Docker Engine usando um componente chamado dockershim porÃ©m essa integraÃ§Ã£o direta nÃ£o estÃ¡ mais disponÃ­vel. A documentaÃ§Ã£o oficial do kubernetes (v1.24) apresenta alguns ambientes de execuÃ§Ã£o e suas respectivas configuraÃ§Ãµes como o [containerd](https://kubernetes.io/docs/setup/production-environment/container-runtimes/#containerd) um projeto avaliado com o nÃ­vel graduado pela CNCF (Cloud Native Computing Foundation) e o [CRI-0](https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cri-o) projeto incubado pela CNCF.

> Projetos graduados e incubados pela CNCF sÃ£o considerados estÃ¡veis â€‹â€‹e utilizados com sucesso em produÃ§Ã£o.

### Portas que devemos nos preocupar

**CONTROL PLANE**

Protocol|Direction|Port Range|Purpose|Used By
--------|---------|----------|-------|-------
TCP|Inbound|6443*|Kubernetes API server|All
TCP|Inbound|2379-2380|etcd server client API|kube-apiserver, etcd
TCP|Inbound|10250|Kubelet API|Self, Control plane
TCP|Inbound|10251|kube-scheduler|Self
TCP|Inbound|10252|kube-controller-manager|Self

* Toda porta marcada por * Ã© customizÃ¡vel, vocÃª precisa se certificar que a porta alterada tambÃ©m esteja aberta.

**WORKERS**

Protocol|Direction|Port Range|Purpose|Used By
--------|---------|----------|-------|-------
TCP|Inbound|10250|Kubelet API|Self, Control plane
TCP|Inbound|30000-32767|NodePort|Services All

Caso vocÃª opte pelo [Weave](https://weave.works) como *pod network*, devem ser liberadas tambÃ©m as portas 6783 (TCP) e 6783/6784 (UDP).

### TÃ¡, mas qual tipo de aplicaÃ§Ã£o eu devo rodar sobre o k8s?

O melhor *app* para executar em contÃªiner, principalmente no k8s, sÃ£o aplicaÃ§Ãµes que seguem o [The Twelve-Factor App](https://12factor.net/pt_br/).

### Conceitos-chave do k8s

Ã‰ importante saber que a forma como o k8s gerencia os contÃªineres Ã© ligeiramente diferente de outros orquestradores, como o Docker Swarm, sobretudo devido ao fato de que ele nÃ£o trata os contÃªineres diretamente, mas sim atravÃ©s de *pods*. Vamos conhecer alguns dos principais conceitos que envolvem o k8s a seguir:

- **Pod**: Ã‰ o menor objeto do k8s. Como dito anteriormente, o k8s nÃ£o trabalha com os contÃªineres diretamente, mas organiza-os dentro de *pods*, que sÃ£o abstraÃ§Ãµes que dividem os mesmos recursos, como endereÃ§os, volumes, ciclos de CPU e memÃ³ria. Um pod pode possuir vÃ¡rios contÃªineres;

- **Deployment**: Ã‰ um dos principais *controllers* utilizados. O *Deployment*, em conjunto com o *ReplicaSet*, garante que determinado nÃºmero de rÃ©plicas de um pod esteja em execuÃ§Ã£o nos nÃ³s workers do cluster. AlÃ©m disso, o Deployment tambÃ©m Ã© responsÃ¡vel por gerenciar o ciclo de vida das aplicaÃ§Ãµes, onde caracterÃ­sticas associadas a aplicaÃ§Ã£o, tais como imagem, porta, volumes e variÃ¡veis de ambiente, podem ser especificados em arquivos do tipo *yaml* ou *json* para posteriormente serem passados como parÃ¢metro para o ``kubectl`` executar o deployment. Esta aÃ§Ã£o pode ser executada tanto para criaÃ§Ã£o quanto para atualizaÃ§Ã£o e remoÃ§Ã£o do deployment;

- **ReplicaSets**: Ã‰ um objeto responsÃ¡vel por garantir a quantidade de pods em execuÃ§Ã£o no nÃ³;

- **Services**: Ã‰ uma forma de vocÃª expor a comunicaÃ§Ã£o atravÃ©s de um *ClusterIP*, *NodePort* ou *LoadBalancer* para distribuir as requisiÃ§Ãµes entre os diversos Pods daquele Deployment. Funciona como um balanceador de carga.

- **Controller**: Ã‰ o objeto responsÃ¡vel por interagir com o *API Server* e orquestrar algum outro objeto. Um exemplo de objeto desta classe Ã© o *Deployments*;

- **Jobs e CronJobs**: sÃ£o objetos responsÃ¡veis pelo gerenciamento de jobs isolados ou recorrentes.


## Importante!

### Aviso sobre os comandos

> **AtenÃ§Ã£o!!!** Antes de cada comando Ã© apresentado o tipo prompt. Exemplos:

```
$ comando1
```

```
# comando2
```

> O prompt que inicia com o caractere "$", indica que o comando deve ser executado com um usuÃ¡rio comum do sistema operacional.
>
> O prompt que inicia com o caractere "#", indica que o comando deve ser executado com o usuÃ¡rio **root**.
>
> VocÃª nÃ£o deve copiar/colar o prompt, apenas o comando. :-)




## Instalando e customizando o Kubectl

### InstalaÃ§Ã£o do Kubectl no GNU/Linux

Vamos instalar o ``kubectl`` com os seguintes comandos.

```
curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl

chmod +x ./kubectl

sudo mv ./kubectl /usr/local/bin/kubectl

kubectl version --client
```

### InstalaÃ§Ã£o do Kubectl no MacOS

O ``kubectl`` pode ser instalado no MacOS utilizando tanto o [Homebrew](https://brew.sh), quanto o mÃ©todo tradicional. Com o Homebrew jÃ¡ instalado, o kubectl pode ser instalado da seguinte forma.

```
sudo brew install kubectl

kubectl version --client
```

Ou:

```
sudo brew install kubectl-cli

kubectl version --client
```

JÃ¡ com o mÃ©todo tradicional, a instalaÃ§Ã£o pode ser realizada com os seguintes comandos.

```
curl -LO "https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/darwin/amd64/kubectl"

chmod +x ./kubectl

sudo mv ./kubectl /usr/local/bin/kubectl

kubectl version --client
```

### InstalaÃ§Ã£o do Kubectl no Windows

A instalaÃ§Ã£o do ``kubectl`` pode ser realizada efetuando o download [neste link](https://dl.k8s.io/release/v1.24.3/bin/windows/amd64/kubectl.exe). 

Outras informaÃ§Ãµes sobre como instalar o kubectl no Windows podem ser encontradas [nesta pÃ¡gina](https://kubernetes.io/docs/tasks/tools/install-kubectl-windows/).


### Customizando o kubectl

#### Auto-complete

Execute o seguinte comando para configurar o alias e autocomplete para o ``kubectl``.

No Bash:

```bash
source <(kubectl completion bash) # configura o autocomplete na sua sessÃ£o atual (antes, certifique-se de ter instalado o pacote bash-completion).

echo "source <(kubectl completion bash)" >> ~/.bashrc # add autocomplete permanentemente ao seu shell.
```

No ZSH:

```bash 
source <(kubectl completion zsh)

echo "[[ $commands[kubectl] ]] && source <(kubectl completion zsh)"
```

#### Criando um alias para o kubectl

Crie o alias ``k`` para ``kubectl``:

```
alias k=kubectl

complete -F __start_kubectl k
```

## Criando um cluster Kubernetes

### Criando o cluster em sua mÃ¡quina local

Vamos mostrar algumas opÃ§Ãµes, caso vocÃª queira comeÃ§ar a brincar com o Kubernetes utilizando somente a sua mÃ¡quina local, o seu desktop.

Lembre-se, vocÃª nÃ£o Ã© obrigado a testar/utilizar todas as opÃ§Ãµes abaixo, mas seria muito bom caso vocÃª testasse. :D

#### Minikube

##### Requisitos bÃ¡sicos

Ã‰ importante frisar que o Minikube deve ser instalado localmente, e nÃ£o em um *cloud provider*. Por isso, as especificaÃ§Ãµes de *hardware* a seguir sÃ£o referentes Ã  mÃ¡quina local.

* Processamento: 1 core;
* MemÃ³ria: 2 GB;
* HD: 20 GB.

##### InstalaÃ§Ã£o do Minikube no GNU/Linux

Antes de mais nada, verifique se a sua mÃ¡quina suporta virtualizaÃ§Ã£o. No GNU/Linux, isto pode ser realizado com o seguinte comando:

```
grep -E --color 'vmx|svm' /proc/cpuinfo
```

Caso a saÃ­da do comando nÃ£o seja vazia, o resultado Ã© positivo.

HÃ¡ a possibilidade de nÃ£o utilizar um *hypervisor* para a instalaÃ§Ã£o do Minikube, executando-o ao invÃ©s disso sobre o prÃ³prio host. Iremos utilizar o Oracle VirtualBox como *hypervisor*, que pode ser encontrado [aqui](https://www.virtualbox.org).

Efetue o download e a instalaÃ§Ã£o do ``Minikube`` utilizando os seguintes comandos.

```
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64

chmod +x ./minikube

sudo mv ./minikube /usr/local/bin/minikube

minikube version
```

##### InstalaÃ§Ã£o do Minikube no MacOS

No MacOS, o comando para verificar se o processador suporta virtualizaÃ§Ã£o Ã©:

```
sysctl -a | grep -E --color 'machdep.cpu.features|VMX'
```

Se vocÃª visualizar `VMX` na saÃ­da, o resultado Ã© positivo.

Efetue a instalaÃ§Ã£o do Minikube com um dos dois mÃ©todos a seguir, podendo optar-se pelo Homebrew ou pelo mÃ©todo tradicional.

```
sudo brew install minikube

minikube version
```

Ou:

```
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-darwin-amd64

chmod +x ./minikube

sudo mv ./minikube /usr/local/bin/minikube

minikube version
```

##### InstalaÃ§Ã£o do Minikube no Microsoft Windows

No Microsoft Windows, vocÃª deve executar o comando `systeminfo` no prompt de comando ou no terminal. Caso o retorno deste comando seja semelhante com o descrito a seguir, entÃ£o a virtualizaÃ§Ã£o Ã© suportada.

```
Hyper-V Requirements:     VM Monitor Mode Extensions: Yes
                          Virtualization Enabled In Firmware: Yes
                          Second Level Address Translation: Yes
                          Data Execution Prevention Available: Yes
```

Caso a linha a seguir tambÃ©m esteja presente, nÃ£o Ã© necessÃ¡ria a instalaÃ§Ã£o de um *hypervisor* como o Oracle VirtualBox:

```
Hyper-V Requirements:     A hypervisor has been detected. Features required for Hyper-V will not be displayed.:     A hypervisor has been detected. Features required for Hyper-V will not be displayed.
```

FaÃ§a o download e a instalaÃ§Ã£o de um *hypervisor* (preferencialmente o [Oracle VirtualBox](https://www.virtualbox.org)), caso no passo anterior nÃ£o tenha sido acusada a presenÃ§a de um. Finalmente, efetue o download do instalador do Minikube [aqui](https://github.com/kubernetes/minikube/releases/latest) e execute-o.


##### Iniciando, parando e excluindo o Minikube

Quando operando em conjunto com um *hypervisor*, o Minikube cria uma mÃ¡quina virtual, onde dentro dela estarÃ£o todos os componentes do k8s para execuÃ§Ã£o.

Ã‰ possÃ­vel selecionar qual *hypervisor* iremos utilizar por padrÃ£o, atravÃ©s no comando abaixo:

```
minikube config set driver <SEU_HYPERVISOR> 
```

VocÃª deve substituir <SEU_HYPERVISOR> pelo seu hypervisor, por exemplo o KVM2, QEMU, Virtualbox ou o Hyperkit.


Caso nÃ£o queria configurar um hypervisor padrÃ£o, vocÃª pode digitar o comando ``minikube start --driver=hyperkit`` toda vez que criar um novo ambiente. 


##### Certo, e como eu sei que estÃ¡ tudo funcionando como deveria?

Uma vez iniciado, vocÃª deve ter uma saÃ­da na tela similar Ã  seguinte:

```
minikube start

ğŸ˜„  minikube v1.26.0 on Debian bookworm/sid
âœ¨  Using the qemu2 (experimental) driver based on user configuration
ğŸ‘  Starting control plane node minikube in cluster minikube
ğŸ”¥  Creating qemu2 VM (CPUs=2, Memory=6000MB, Disk=20000MB) ...
ğŸ³  Preparing Kubernetes v1.24.1 on Docker 20.10.16 ...
    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ğŸ”  Verifying Kubernetes components...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ğŸŒŸ  Enabled addons: default-storageclass, storage-provisioner
ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

```

VocÃª pode entÃ£o listar os nÃ³s que fazem parte do seu *cluster* k8s com o seguinte comando:

```
kubectl get nodes
```

A saÃ­da serÃ¡ similar ao conteÃºdo a seguir:

```
kubectl get nodes
NAME       STATUS   ROLES           AGE   VERSION
minikube   Ready    control-plane   26s   v1.24.1
```

Para criar um cluster com mais de um nÃ³, vocÃª pode utilizar o comando abaixo, apenas modificando os valores para o desejado:

```
minikube start --nodes 2 -p multinode-cluster

ğŸ˜„  minikube v1.26.0 on Debian bookworm/sid
âœ¨  Automatically selected the docker driver. Other choices: kvm2, virtualbox, ssh, none, qemu2 (experimental)
ğŸ“Œ  Using Docker driver with root privileges
ğŸ‘  Starting control plane node minikube in cluster minikube
ğŸšœ  Pulling base image ...
ğŸ’¾  Downloading Kubernetes v1.24.1 preload ...
    > preloaded-images-k8s-v18-v1...: 405.83 MiB / 405.83 MiB  100.00% 66.78 Mi
    > gcr.io/k8s-minikube/kicbase: 385.99 MiB / 386.00 MiB  100.00% 23.63 MiB p
    > gcr.io/k8s-minikube/kicbase: 0 B [_________________________] ?% ? p/s 11s
ğŸ”¥  Creating docker container (CPUs=2, Memory=8000MB) ...
ğŸ³  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
    â–ª Generating certificates and keys ...
    â–ª Booting up control plane ...
    â–ª Configuring RBAC rules ...
ğŸ”—  Configuring CNI (Container Networking Interface) ...
ğŸ”  Verifying Kubernetes components...
    â–ª Using image gcr.io/k8s-minikube/storage-provisioner:v5
ğŸŒŸ  Enabled addons: storage-provisioner, default-storageclass

ğŸ‘  Starting worker node minikube-m02 in cluster minikube
ğŸšœ  Pulling base image ...
ğŸ”¥  Creating docker container (CPUs=2, Memory=8000MB) ...
ğŸŒ  Found network options:
    â–ª NO_PROXY=192.168.11.11
ğŸ³  Preparing Kubernetes v1.24.1 on Docker 20.10.17 ...
    â–ª env NO_PROXY=192.168.11.11
ğŸ”  Verifying Kubernetes components...
ğŸ„  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

```

Para visualizar os nÃ³s do seu novo cluster Kubernetes, digite:

```
kubectl get nodes

NAME           STATUS   ROLES           AGE   VERSION
minikube       Ready    control-plane   66s   v1.24.1
minikube-m02   Ready    <none>          48s   v1.24.1
```

Inicialmente, a intenÃ§Ã£o do Minikube Ã© executar o k8s em apenas um nÃ³, porÃ©m a partir da versÃ£o 1.10.1 e possÃ­vel usar a funÃ§Ã£o de multi-node.

Caso os comandos anteriores tenham sido executados sem erro, a instalaÃ§Ã£o do Minikube terÃ¡ sido realizada com sucesso.

##### Ver detalhes sobre o cluster 

```
minikube status
```

##### Descobrindo o endereÃ§o do Minikube

Como dito anteriormente, o Minikube irÃ¡ criar uma mÃ¡quina virtual, assim como o ambiente para a execuÃ§Ã£o do k8s localmente. Ele tambÃ©m irÃ¡ configurar o ``kubectl`` para comunicar-se com o Minikube. Para saber qual Ã© o endereÃ§o IP dessa mÃ¡quina virtual, pode-se executar:

```
minikube ip
```

O endereÃ§o apresentado Ã© que deve ser utilizado para comunicaÃ§Ã£o com o k8s.

##### Acessando a mÃ¡quina do Minikube via SSH

Para acessar a mÃ¡quina virtual criada pelo Minikube, pode-se executar:

```
minikube ssh
```

##### Dashboard

O Minikube vem com um *dashboard* *web* interessante para que o usuÃ¡rio iniciante observe como funcionam os *workloads* sobre o k8s. Para habilitÃ¡-lo, o usuÃ¡rio pode digitar:

```
minikube dashboard
```

##### Logs

Os *logs* do Minikube podem ser acessados atravÃ©s do seguinte comando.

```
minikube logs
```

##### Remover o cluster

```
minikube delete
```

Caso queira remover o cluster e todos os arquivos referente a ele, utilize o parametro *--purge*, conforme abaixo:

```
minikube delete --purge
```

#### Kind

O Kind (*Kubernetes in Docker*) Ã© outra alternativa para executar o Kubernetes num ambiente local para testes e aprendizado, mas nÃ£o Ã© recomendado para uso em produÃ§Ã£o.

##### InstalaÃ§Ã£o no GNU/Linux

Para fazer a instalaÃ§Ã£o no GNU/Linux, execute os seguintes comandos.

```
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.16.0/kind-linux-amd64

chmod +x ./kind

sudo mv ./kind /usr/local/bin/kind
```

##### InstalaÃ§Ã£o no MacOS

Para fazer a instalaÃ§Ã£o no MacOS, execute o seguinte comando.

```
sudo brew install kind
```

ou

```
Para Intel Macs
[ $(uname -m) = x86_64 ]&& curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.16.0/kind-darwin-amd64
Para M1 / ARM Macs
[ $(uname -m) = arm64 ] && curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.16.0/kind-darwin-arm64

chmod +x ./kind
mv ./kind /usr/bin/kind
```

##### InstalaÃ§Ã£o no Windows

Para fazer a instalaÃ§Ã£o no Windows, execute os seguintes comandos.

```
curl.exe -Lo kind-windows-amd64.exe https://kind.sigs.k8s.io/dl/v0.14.0/kind-windows-amd64

Move-Item .\kind-windows-amd64.exe c:\kind.exe
```

###### InstalaÃ§Ã£o no Windows via [Chocolatey](https://chocolatey.org/install)

Execute o seguinte comando para instalar o Kind no Windows usando o Chocolatey.

```
choco install kind
```

##### Criando um cluster com o Kind

ApÃ³s realizar a instalaÃ§Ã£o do Kind, vamos iniciar o nosso cluster.

```
kind create cluster

Creating cluster "kind" ...
 âœ“ Ensuring node image (kindest/node:v1.25.2) ğŸ–¼
 âœ“ Preparing nodes ğŸ“¦  
 âœ“ Writing configuration ğŸ“œ 
 âœ“ Starting control-plane ğŸ•¹ï¸ 
 âœ“ Installing CNI ğŸ”Œ 
 âœ“ Installing StorageClass ğŸ’¾ 
Set kubectl context to "kind-kind"
You can now use your cluster with:

kubectl cluster-info --context kind-kind

Not sure what to do next? ğŸ˜…  Check out https://kind.sigs.k8s.io/docs/user/quick-start/

```

Ã‰ possÃ­vel criar mais de um cluster e personalizar o seu nome.

```
kind create cluster --name giropops

Creating cluster "giropops" ...
 âœ“ Ensuring node image (kindest/node:v1.25.2) ğŸ–¼
 âœ“ Preparing nodes ğŸ“¦  
 âœ“ Writing configuration ğŸ“œ 
 âœ“ Starting control-plane ğŸ•¹ï¸ 
 âœ“ Installing CNI ğŸ”Œ 
 âœ“ Installing StorageClass ğŸ’¾ 
Set kubectl context to "kind-giropops"
You can now use your cluster with:

kubectl cluster-info --context kind-giropops

Thanks for using kind! ğŸ˜Š
```

Para visualizar os seus clusters utilizando o kind, execute o comando a seguir.

```
kind get clusters

kind
giropops
```

Liste os nodes do cluster.

```
kubectl get nodes

NAME                     STATUS   ROLES           AGE   VERSION
giropops-control-plane   Ready    control-plane   74s   v1.25.2

```

##### Criando um cluster com mÃºltiplos nÃ³s locais com o Kind

Ã‰ possÃ­vel para essa aula incluir mÃºltiplos nÃ³s na estrutura do Kind, que foi mencionado anteriormente.

Execute o comando a seguir para selecionar e remover todos os clusters locais criados no Kind.

```
kind delete clusters $(kind get clusters)

Deleted clusters: ["giropops" "kind"]
```

Crie um arquivo de configuraÃ§Ã£o para definir quantos e o tipo de nÃ³s no cluster que vocÃª deseja. No exemplo a seguir, serÃ¡ criado o arquivo de configuraÃ§Ã£o ``kind-3nodes.yaml`` para especificar um cluster com 1 nÃ³ control-plane (que executarÃ¡ o control plane) e 2 workers.

```
cat << EOF > $HOME/kind-3nodes.yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
  - role: control-plane
  - role: worker
  - role: worker
EOF
```

Agora vamos criar um cluster chamado ``kind-multinodes`` utilizando as especificaÃ§Ãµes definidas no arquivo ``kind-3nodes.yaml``.

```
kind create cluster --name kind-multinodes --config $HOME/kind-3nodes.yaml

Creating cluster "kind-multinodes" ...
 âœ“ Ensuring node image (kindest/node:v1.25.2) ğŸ–¼
 âœ“ Preparing nodes ğŸ“¦ ğŸ“¦ ğŸ“¦  
 âœ“ Writing configuration ğŸ“œ 
 âœ“ Starting control-plane ğŸ•¹ï¸ 
 âœ“ Installing CNI ğŸ”Œ 
 âœ“ Installing StorageClass ğŸ’¾ 
 âœ“ Joining worker nodes ğŸšœ 
Set kubectl context to "kind-kind-multinodes"
You can now use your cluster with:

kubectl cluster-info --context kind-kind-multinodes

Have a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community ğŸ™‚
```

Valide a criaÃ§Ã£o do cluster com o comando a seguir.

```
kubectl get nodes

NAME                            STATUS   ROLES           AGE   VERSION
kind-multinodes-control-plane   Ready    control-plane   52s   v1.25.2
kind-multinodes-worker          Ready    <none>          32s   v1.25.2
kind-multinodes-worker2         Ready    <none>          32s   v1.25.2
```

Mais informaÃ§Ãµes sobre o Kind estÃ£o disponÃ­veis em: https://kind.sigs.k8s.io


### InstalaÃ§Ã£o do cluster Kubernetes em trÃªs nÃ³s

#### Requisitos bÃ¡sicos

Como jÃ¡ dito anteriormente, o Minikube Ã© Ã³timo para desenvolvedores, estudos e testes, mas nÃ£o tem como propÃ³sito a execuÃ§Ã£o em ambiente de produÃ§Ã£o. Dito isso, a instalaÃ§Ã£o de um *cluster* k8s para o treinamento irÃ¡ requerer pelo menos trÃªs mÃ¡quinas, fÃ­sicas ou virtuais, cada qual com no mÃ­nimo a seguinte configuraÃ§Ã£o:

- DistribuiÃ§Ã£o: Debian, Ubuntu, CentOS, Red Hat, Fedora, SuSE;

- Processamento: 2 *cores*;

- MemÃ³ria: 2GB.

#### ConfiguraÃ§Ã£o de mÃ³dulos e parametrizaÃ§Ã£o de kernel

O k8s requer que certos mÃ³dulos do kernel GNU/Linux estejam carregados para seu pleno funcionamento, e que esses mÃ³dulos sejam carregados no momento da inicializaÃ§Ã£o do computador. Para tanto, crie o arquivo ``/etc/modules-load.d/k8s.conf`` com o seguinte conteÃºdo em todos os seus nÃ³s.

```bash
vim /etc/modules-load.d/k8s.conf 
```

```
br_netfilter
ip_vs
ip_vs_rr
ip_vs_sh
ip_vs_wrr
nf_conntrack_ipv4
overlay
```

Vamos habilitar o repasse de pacotes e fazer com que o *iptables* gerencie os pacotes que estÃ£o trafegando pelas *brigdes*. Para isso vamos utilizar *systcl* para parametrizar o kernel.

```bash
vim /etc/sysctl.d/k8s.conf 
```

```
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1
```

Para ler as novas configuraÃ§Ãµes:

```bash
sysctl --system
```

#### AtualizaÃ§Ã£o da distribuiÃ§Ã£o

Em distribuiÃ§Ãµes Debian e baseadas, como o Ubuntu, execute os comandos a seguir, em cada um de seus nÃ³s, para executar atualizaÃ§Ã£o do sistema.

```
sudo apt update

sudo apt upgrade -y
```

Em distribuiÃ§Ãµes Red Hat e baseadas, use o seguinte comando.

```
sudo yum upgrade -y
```

#### O Container Runtime

Para que seja possÃ­vel executar os containers nos nÃ³s Ã© necessÃ¡rio ter um *container runtime* instalado em cada um dos nÃ³s.

O *container runtime* ou o *container engine* Ã© o responsÃ¡vel por executar os containers nos nÃ³s. Quando vocÃª estÃ¡ utilizando containers em sua mÃ¡quina, por exemplo, vocÃª estÃ¡ fazendo uso de algum *container runtime*.

O *container runtime* Ã© o responsÃ¡vel por gerenciar as imagens e volumes, Ã© ele o responsÃ¡vel por garantir que os os recursos que os containers estÃ£o utilizando estÃ¡ devidamente isolados, a vida do container e muito mais.

Hoje temos diversas opÃ§Ãµes para se utilizar como *container runtime*, que atÃ© pouco tempo atrÃ¡s tinhamos somente o Docker para esse papel.

Hoje o Docker nÃ£o Ã© mais suportado pelo Kubernetes, pois o Docker Ã© muito mais do que apenas um *container runtime*. 

O Docker Swarm, por exemplo, vem por padrÃ£o quando vocÃª instala o Docker, ou seja, nÃ£o faz sentido ter o Docker inteiro sendo que o Kubernetes somente utiliza um pedaÃ§o pequeno do Docker.

O Kubernetes suporta diversos *container runtime*, desde que alinhados com o *Open Container Interface*, o OCI.

*Container runtimes* suportados pelo Kubernetes:

- containerd
- CRI-O
- Docker Engine
- Mirantis Container Runtime


##### Instalando e configurando o containerd

Para instalar o *containerd* nos nÃ³s, utilize o instalador de pacotes padrÃ£o de sua distribuiÃ§Ã£o. Para esse exemplo estou utilizando um Ubuntu Server, entÃ£o irei utilizar o *apt*.

Para que isso seja possÃ­vel vamos adicionar o repositÃ³rio do Docker. Mas nÃ³s nÃ£o iremos instalar o Docker, iremos somente realizar a instalaÃ§Ã£o do Containerd.


```bash
# Adicionando a chave do repositÃ³rio
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - 

sudo add-apt-repository "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
sudo apt update

# Instalando o containerd
sudo apt install -y containerd.io
```

Agora vamos criar diretÃ³rio que irÃ¡ conter as configuraÃ§Ãµes do *containerd*.

```bash
mkdir -p /etc/containerd
```

Agora jÃ¡ podemos criar a configuraÃ§Ã£o bÃ¡sica para o nosso *containerd*, lembrando que Ã© super importante ler a documentaÃ§Ã£o do *containerd* para que vocÃª possa conhecer todas as opÃ§Ãµes para o seu ambiente.

```bash
containerd config default > /etc/containerd/config.toml
```

Agora vamos reiniciar o serviÃ§o para que as novas configuraÃ§Ãµes entrem em vigor.

```bash
# Habilitando o serviÃ§o
systemctl enable containerd

# Reiniciando o serviÃ§o
systemctl restart containerd
```

##### Instalando o kubeadm

O prÃ³ximo passo Ã© efetuar a adiÃ§Ã£o dos repositÃ³rios do k8s e efetuar a instalaÃ§Ã£o do ``kubeadm``.

Em distribuiÃ§Ãµes Debian e baseadas, isso pode ser realizado com os comandos a seguir.

```
sudo apt-get update && sudo apt-get install -y apt-transport-https gnupg2

curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -

sudo echo "deb http://apt.kubernetes.io/ kubernetes-xenial main" > /etc/apt/sources.list.d/kubernetes.list

sudo apt-get update

sudo apt-get install -y kubelet kubeadm kubectl
```

Ã‰ necessÃ¡rio desativar a memÃ³ria swap em todos os nÃ³s com o comando a seguir.

```bash
sudo swapoff -a
```

AlÃ©m de comentar a linha referente Ã  mesma no arquivo ```/etc/fstab```.

```bash
sudo sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab
```

ApÃ³s esses procedimentos, Ã© interessante a reinicializaÃ§Ã£o de todos os nÃ³s do *cluster*.


##### InicializaÃ§Ã£o do cluster

Antes de inicializarmos o *cluster*, vamos efetuar o *download* das imagens que serÃ£o utilizadas, executando o comando a seguir no nÃ³ que serÃ¡ o *control-plane*.
Vamos passar o parametro *--cri-socket* para especificar o caminho do arquivo de socket do nosso *container runtime*, nesse caso o *containerd*

```
sudo kubeadm config images pull --cri-socket /run/containerd/containerd.sock
```

Execute o comando a seguir tambÃ©m apenas no nÃ³ *control-plane* para a inicializaÃ§Ã£o do cluster. 

Estamos passando alguns importantes parametros:

- _--control-plane-endpoint_ -> Ip do seu node que serÃ¡ utilizado no cluster. Importante caso vocÃª tenha mais de uma interface ou endereÃ§o.

- _--cri-socket_ -> O arquivo de socket do nosso container runtime.

- _--upload-certs_ -> Faz o upload do certificado do *control plane* para o kubeadm-certs  secret.


```
sudo kubeadm init --upload-certs --control-plane-endpoint=ADICIONE_O_IP_DO_NODE_AQUI  --cri-socket /run/containerd/containerd.sock
```

Opcionalmente, vocÃª tambÃ©m pode passar o cidr com a opÃ§Ã£o _--pod-network-cidr_. O comando obedecerÃ¡ a seguinte sintaxe:

```
sudo kubeadm init --upload-certs --control-plane-endpoint=ADICIONE_O_IP_DO_NODE_AQUI  --cri-socket /run/containerd/containerd.sock --pod-network-cidr 192.168.99.0/24
```

A saÃ­da do comando serÃ¡ algo similar ao mostrado a seguir.

```
[init] Using Kubernetes version: v1.24.3
[preflight] Running pre-flight checks
[preflight] Pulling images required for setting up a Kubernetes cluster
[preflight] This might take a minute or two, depending on the speed of your internet connection
[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
[certs] Using certificateDir folder "/etc/kubernetes/pki"
[certs] Generating "ca" certificate and key
[certs] Generating "apiserver" certificate and key
[certs] apiserver serving cert is signed for DNS names [172.31.19.147 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.31.19.147]
[certs] Generating "apiserver-kubelet-client" certificate and key
[certs] Generating "front-proxy-ca" certificate and key
[certs] Generating "front-proxy-client" certificate and key
[certs] Generating "etcd/ca" certificate and key
[certs] Generating "etcd/server" certificate and key
[certs] etcd/server serving cert is signed for DNS names [172.31.19.147 localhost] and IPs [172.31.19.147 127.0.0.1 ::1]
[certs] Generating "etcd/peer" certificate and key
[certs] etcd/peer serving cert is signed for DNS names [172.31.19.147 localhost] and IPs [172.31.19.147 127.0.0.1 ::1]
[certs] Generating "etcd/healthcheck-client" certificate and key
[certs] Generating "apiserver-etcd-client" certificate and key
[certs] Generating "sa" key and public key
[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
[kubeconfig] Writing "admin.conf" kubeconfig file
[kubeconfig] Writing "kubelet.conf" kubeconfig file
[kubeconfig] Writing "controller-manager.conf" kubeconfig file
[kubeconfig] Writing "scheduler.conf" kubeconfig file
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Starting the kubelet
[control-plane] Using manifest folder "/etc/kubernetes/manifests"
[control-plane] Creating static Pod manifest for "kube-apiserver"
[control-plane] Creating static Pod manifest for "kube-controller-manager"
[control-plane] Creating static Pod manifest for "kube-scheduler"
[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
[apiclient] All control plane components are healthy after 8.505808 seconds
[upload-config] Storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
[kubelet] Creating a ConfigMap "kubelet-config" in namespace kube-system with the configuration for the kubelets in the cluster
[upload-certs] Storing the certificates in Secret "kubeadm-certs" in the "kube-system" Namespace
[upload-certs] Using certificate key:
55befb249a01aca7be98b3e7209628f4c4f04c6a05c250c4bb084af722452c36
[mark-control-plane] Marking the node 172.31.19.147 as control-plane by adding the labels: [node-role.kubernetes.io/control-plane node.kubernetes.io/exclude-from-external-load-balancers]
[mark-control-plane] Marking the node 172.31.19.147 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule node-role.kubernetes.io/control-plane:NoSchedule]
[bootstrap-token] Using token: q1m5ci.5p2mtgby0s4ek4vr
[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to get nodes
[bootstrap-token] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
[bootstrap-token] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
[bootstrap-token] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
[bootstrap-token] Creating the "cluster-info" ConfigMap in the "kube-public" namespace
[kubelet-finalize] Updating "/etc/kubernetes/kubelet.conf" to point to a rotatable kubelet client certificate and key
[addons] Applied essential addon: CoreDNS
[addons] Applied essential addon: kube-proxy

Your Kubernetes control-plane has initialized successfully!

To start using your cluster, you need to run the following as a regular user:

  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

Alternatively, if you are the root user, you can run:

  export KUBECONFIG=/etc/kubernetes/admin.conf

You should now deploy a pod network to the cluster.
Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
  https://kubernetes.io/docs/concepts/cluster-administration/addons/

You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 172.31.19.147:6443 --token q1m5ci.5p2mtgby0s4ek4vr \
	--discovery-token-ca-cert-hash sha256:45f6437514981d97631bd5d48822c670ec4a548c9768043fca6e5eda0133b934 \
	--control-plane --certificate-key 55befb249a01aca7be98b3e7209628f4c4f04c6a05c250c4bb084af722452c36

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.31.19.147:6443 --token q1m5ci.5p2mtgby0s4ek4vr \
	--discovery-token-ca-cert-hash sha256:45f6437514981d97631bd5d48822c670ec4a548c9768043fca6e5eda0133b934 
```

Caso o servidor possua mais de uma interface de rede, vocÃª pode verificar se o IP interno do nÃ³ do seu cluster corresponde ao IP da interface esperada com o seguinte comando:

```
kubectl describe node elliot-1 | grep InternalIP
```

A saÃ­da serÃ¡ algo similar a seguir:

```
InternalIP:  172.31.19.147
```

Caso o IP nÃ£o corresponda ao da interface de rede escolhida, vocÃª pode ir atÃ© o arquivo localizado em _/etc/systemd/system/kubelet.service.d/10-kubeadm.conf_ com o editor da sua preferÃªncia, procure por _KUBELET_CONFIG_ARGS_ e adicione no final a instruÃ§Ã£o --node-ip=<IP Da sua preferÃªncia>. O trecho alterado serÃ¡ semelhante a esse:

```
Environment="KUBELET_CONFIG_ARGS=--config=/var/lib/kubelet/config.yaml --node-ip=192.168.99.2"
```

Salve o arquivo e execute os comandos abaixo para reiniciar a configuraÃ§Ã£o e consequentemente o kubelet.

```
sudo systemctl daemon-reload
sudo systemctl restart kubelet
```

#### ConfiguraÃ§Ã£o do arquivo de contextos do kubectl

Como dito anteriormente e de forma similar ao Docker Swarm, o prÃ³prio kubeadm jÃ¡ mostrarÃ¡ os comandos necessÃ¡rios para a configuraÃ§Ã£o do ``kubectl``, para que assim possa ser estabelecida comunicaÃ§Ã£o com o cluster k8s. Para tanto, execute os seguintes comandos.

```
mkdir -p $HOME/.kube

cp -i /etc/kubernetes/admin.conf $HOME/.kube/config

sudo chown $(id -u):$(id -g) $HOME/.kube/config
```

#### Inserindo os nÃ³s workers no cluster

Para inserir os nÃ³s *workers* ou mais *control plane* no *cluster*, basta executar a linha que comeÃ§a com ``kubeadm join`` que vimos na saÃ­da do comando de inicializaÃ§Ã£o do cluster.

```
You can now join any number of the control-plane node running the following command on each as root:

  kubeadm join 172.31.19.147:6443 --token q1m5ci.5p2mtgby0s4ek4vr \
	--discovery-token-ca-cert-hash sha256:45f6437514981d97631bd5d48822c670ec4a548c9768043fca6e5eda0133b934 \
	--control-plane --certificate-key 55befb249a01aca7be98b3e7209628f4c4f04c6a05c250c4bb084af722452c36

Please note that the certificate-key gives access to cluster sensitive data, keep it secret!
As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use
"kubeadm init phase upload-certs --upload-certs" to reload certs afterward.

Then you can join any number of worker nodes by running the following on each as root:

kubeadm join 172.31.19.147:6443 --token q1m5ci.5p2mtgby0s4ek4vr \
	--discovery-token-ca-cert-hash sha256:45f6437514981d97631bd5d48822c670ec4a548c9768043fca6e5eda0133b934 
```

Conforme notamos na saÃ­da acima temos dois comandos, um para que possamos adicionar mais nÃ³s como *control plane* ou entÃ£o para adicionar nÃ³s como *worker*.

Apenas copie e cole o comando nos nÃ³s que vocÃª deseja adicionar ao cluster. Nessa linha de comando do *kubeadm join* jÃ¡ estamos passando o IP e porta do nosso primeiro nÃ³ *control plane* e as informaÃ§Ãµes sobre o certificado, informaÃ§Ãµes necessÃ¡rias para que seja possÃ­vel a entrada do nÃ³ no cluster.


Lembre-se, o comando abaixo deve ser executado nos nÃ³s que irÃ£o compor o cluster, no exemplo vamos adicionar mais dois nÃ³s como *workers*

```bash
kubeadm join 172.31.19.147:6443 --token q1m5ci.5p2mtgby0s4ek4vr --discovery-token-ca-cert-hash sha256:45f6437514981d97631bd5d48822c670ec4a548c9768043fca6e5eda0133b934 

[preflight] Running pre-flight checks
[preflight] Reading configuration from the cluster...
[preflight] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -o yaml'
[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
[kubelet-start] Starting the kubelet
[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...

This node has joined the cluster:
* Certificate signing request was sent to apiserver and a response was received.
* The Kubelet was informed of the new secure connection details.

Run 'kubectl get nodes' on the control-plane to see this node join the cluster.
```

Agora no nÃ³ *control plane* verifique os nÃ³s que jÃ¡ fazem parte do cluster atravÃ©s do comando *kubectl*

```bash
kubectl get nodes
NAME               STATUS     ROLES           AGE   VERSION
ip-172-31-19-147   NotReady   control-plane   68s   v1.24.3
ip-172-31-24-77    NotReady   <none>          29s   v1.24.3
ip-172-31-25-32    NotReady   <none>          31s   v1.24.3
```

Perceba que os nÃ³s ainda nÃ£o estÃ£o *Ready*, pois ainda nÃ£o instalamos o *pod network* para resolver a comunicaÃ§Ã£o entre pods em diferentes nÃ³s.

#### A rede do Kubernetes

Entender como funciona a rede no Kubernetes Ã© super importante para que vocÃª consiga entender nÃ£o somente o comportamento do prÃ³prio Kubernetes, como tambÃ©m para o entendimento de como as suas aplicaÃ§Ãµes se comportam e interagem.
Primeira coisa que devemos entender Ã© que o Kubernetes nÃ£o resolve como funciona a comunicaÃ§Ã£o de pods em nÃ³s diferentes, para que isso seja resolvido Ã© necessÃ¡rio utilizar o que chamamos de *pod networking*.

Ou seja, o k8s por padrÃ£o nÃ£o fornece uma soluÃ§Ã£o de *networking* *out-of-the-box*. 

Para resolver esse problema foi criado o *Container Network Interface*, o **CNI**.
O *CNI* nada mais Ã© do que uma especificaÃ§Ã£o e um conjunto de bibliotecas para a criaÃ§Ã£o de soluÃ§Ãµes de *pod networking*, ou seja, plugins para resolver o problema de comunicaÃ§Ã£o entre os pods.

Temos diversas soluÃ§Ã£o de *pod networking* como *add-on*, cada qual com funcionalidades diferentes, tais como: [Flannel](https://github.com/coreos/flannel), [Calico](http://docs.projectcalico.org/), [Romana](http://romana.io), [Weave-net](https://www.weave.works/products/weave-net/), entre outros.

Ã‰ importante saber as caracteristicas de cada soluÃ§Ã£o e como elas resolvem a comunicaÃ§Ã£o entre os pods.

Por exemplo, temos soluÃ§Ãµes que utilizam *eBPF* como Ã© o caso do *Cilium*, ou ainda soluÃ§Ãµes que atuam na camada 3 ou na camada 7 do modelo de referencia OSI. 

Dito isso, a melhor coisa Ã© vocÃª ler os detalhes de cada soluÃ§Ã£o e entender qual a melhor antende suas necessidades.

Eu gosto muito da **Weave-net** e serÃ¡ ela que iremos abordar durante o treinamento, na dÃºvida de qual usar, vÃ¡ de **Weave-net**! :)


Para instalar o *Weave-net* execute o seguinte comando no nÃ³ *control plane*.

```
kubectl apply -f "https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\n')"
```

Para verificar se o *pod network* foi criado com sucesso, execute o seguinte comando.

```
kubectl get pods -n kube-system
```

O resultado deve ser semelhante ao mostrado a seguir.

```
NAMESPACE     NAME                                       READY   STATUS    RESTARTS      AGE
kube-system   coredns-6d4b75cb6d-vjtw5                   1/1     Running   0             2m4s
kube-system   coredns-6d4b75cb6d-xd89l                   1/1     Running   0             2m4s
kube-system   etcd-ip-172-31-19-147                      1/1     Running   0             2m19s
kube-system   kube-apiserver-ip-172-31-19-147            1/1     Running   0             2m18s
kube-system   kube-controller-manager-ip-172-31-19-147   1/1     Running   0             2m18s
kube-system   kube-proxy-djvp4                           1/1     Running   0             103s
kube-system   kube-proxy-f2f57                           1/1     Running   0             2m5s
kube-system   kube-proxy-tshff                           1/1     Running   0             105s
kube-system   kube-scheduler-ip-172-31-19-147            1/1     Running   0             2m18s
kube-system   weave-net-4qfbb                            2/2     Running   1 (22s ago)   28s
kube-system   weave-net-htlrp                            2/2     Running   1 (22s ago)   28s
kube-system   weave-net-nltmv                            2/2     Running   1 (21s ago)   28s

```

Pode-se observar que hÃ¡ trÃªs contÃªineres do Weave-net em execuÃ§Ã£o, um em cada nÃ³ do cluster,  provendo a *pod networking* para o nosso *cluster*.

#### Verificando a instalaÃ§Ã£o

Para verificar se a instalaÃ§Ã£o estÃ¡ funcionando, e se os nÃ³s estÃ£o se comunicando, vocÃª pode executar o comando ``kubectl get nodes`` no nÃ³ control-plane, que deve lhe retornar algo como o conteÃºdo a seguir.


```bash
kubectl get nodes

NAME               STATUS   ROLES           AGE     VERSION
ip-172-31-19-147   Ready    control-plane   2m20s   v1.24.3
ip-172-31-24-77    Ready    <none>          101s    v1.24.3
ip-172-31-25-32    Ready    <none>          103s    v1.24.3

```

### Primeiros passos no k8s

#### Exibindo informaÃ§Ãµes detalhadas sobre os nÃ³s

```
kubectl describe node [nome_do_no]
```

Exemplo:

```
kubectl describe node ip-172-31-19-147

Name:               ip-172-31-19-147
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=ip-172-31-19-147
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sun, 07 Aug 2022 07:05:52 +0000
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
                    node-role.kubernetes.io/master:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  ip-172-31-19-147
  AcquireTime:     <unset>
  RenewTime:       Sun, 07 Aug 2022 08:10:33 +0000
Conditions:
  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----                 ------  -----------------                 ------------------                ------                       -------
  NetworkUnavailable   False   Sun, 07 Aug 2022 07:07:56 +0000   Sun, 07 Aug 2022 07:07:56 +0000   WeaveIsUp                    Weave pod has set this
  MemoryPressure       False   Sun, 07 Aug 2022 08:09:15 +0000   Sun, 07 Aug 2022 07:05:49 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure         False   Sun, 07 Aug 2022 08:09:15 +0000   Sun, 07 Aug 2022 07:05:49 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure          False   Sun, 07 Aug 2022 08:09:15 +0000   Sun, 07 Aug 2022 07:05:49 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready                True    Sun, 07 Aug 2022 08:09:15 +0000   Sun, 07 Aug 2022 07:07:58 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled
Addresses:
  InternalIP:  172.31.19.147
  Hostname:    ip-172-31-19-147
Capacity:
  cpu:                2
  ephemeral-storage:  7950756Ki
  hugepages-2Mi:      0
  memory:             4016852Ki
  pods:               110
Allocatable:
  cpu:                2
  ephemeral-storage:  7327416718
  hugepages-2Mi:      0
  memory:             3914452Ki
  pods:               110
System Info:
  Machine ID:                 23fb437f79c4489ab1e351f42b69a52c
  System UUID:                ec2e1b61-092b-df48-4c41-f51d2f5e84d7
  Boot ID:                    1e1ce6a2-3cf0-4961-be37-1f15ba5cd232
  Kernel Version:             5.13.0-1029-aws
  OS Image:                   Ubuntu 20.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.6.6
  Kubelet Version:            v1.24.3
  Kube-Proxy Version:         v1.24.3
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (8 in total)
....
```

##### Exibindo novamente token para adicionar um novo nÃ³ no cluster

Para visualizar novamente o *token* para inserÃ§Ã£o de novos nÃ³s, execute o seguinte comando.

```
sudo kubeadm token create --print-join-command
```

##### Ativando o autocomplete

Em distribuiÃ§Ãµes Debian e baseadas, certifique-se que o pacote ``bash-completion`` esteja instalado. Instale-o com o comando a seguir.

```
sudo apt install -y bash-completion
```

Em sistemas Red Hat e baseados, execute:

```
sudo yum install -y bash-completion
```

Feito isso, execute o seguinte comando.

```
kubectl completion bash > /etc/bash_completion.d/kubectl
```

Efetue *logoff* e *login* para carregar o *autocomplete*. Caso nÃ£o deseje, execute:

```
source <(kubectl completion bash)
```

##### Verificando os namespaces e pods

O k8s organiza tudo dentro de *namespaces*. Por meio deles, podem ser realizadas limitaÃ§Ãµes de seguranÃ§a e de recursos dentro do *cluster*, tais como *pods*, *replication controllers* e diversos outros. Para visualizar os *namespaces* disponÃ­veis no *cluster*, digite:

```
kubectl get namespaces

NAME              STATUS   AGE
default           Active   8d
kube-node-lease   Active   8d
kube-public       Active   8d
kube-system       Active   8d
```

Vamos listar os *pods* do *namespace* **kube-system** utilizando o comando a seguir.

```
kubectl get pod -n kube-system

NAME                                       READY   STATUS    RESTARTS       AGE
coredns-6d4b75cb6d-vjtw5                   1/1     Running   0              106m
coredns-6d4b75cb6d-xd89l                   1/1     Running   0              106m
etcd-ip-172-31-19-147                      1/1     Running   0              106m
kube-apiserver-ip-172-31-19-147            1/1     Running   0              106m
kube-controller-manager-ip-172-31-19-147   1/1     Running   0              106m
kube-proxy-djvp4                           1/1     Running   0              106m
kube-proxy-f2f57                           1/1     Running   0              106m
kube-proxy-tshff                           1/1     Running   0              106m
kube-scheduler-ip-172-31-19-147            1/1     Running   0              106m
weave-net-4qfbb                            2/2     Running   1 (104m ago)   105m
weave-net-htlrp                            2/2     Running   1 (104m ago)   105m
weave-net-nltmv                            2/2     Running   1 (104m ago)   105m
```

SerÃ¡ que hÃ¡ algum *pod* escondido em algum *namespace*? Ã‰ possÃ­vel listar todos os *pods* de todos os *namespaces* com o comando a seguir.

```
kubectl get pods --all-namespaces
```

HÃ¡ a possibilidade ainda, de utilizar o comando com a opÃ§Ã£o ```-o wide```, que disponibiliza maiores informaÃ§Ãµes sobre o recurso, inclusive em qual nÃ³ o *pod* estÃ¡ sendo executado. Exemplo:

```
kubectl get pods --all-namespaces -o wide

NAMESPACE     NAME                                       READY   STATUS    RESTARTS       AGE    IP              NODE               NOMINATED NODE   READINESS GATES
kube-system   coredns-6d4b75cb6d-vjtw5                   1/1     Running   0              105m   10.32.0.3       ip-172-31-19-147   <none>           <none>
kube-system   coredns-6d4b75cb6d-xd89l                   1/1     Running   0              105m   10.32.0.2       ip-172-31-19-147   <none>           <none>
kube-system   etcd-ip-172-31-19-147                      1/1     Running   0              105m   172.31.19.147   ip-172-31-19-147   <none>           <none>
kube-system   kube-apiserver-ip-172-31-19-147            1/1     Running   0              105m   172.31.19.147   ip-172-31-19-147   <none>           <none>
kube-system   kube-controller-manager-ip-172-31-19-147   1/1     Running   0              105m   172.31.19.147   ip-172-31-19-147   <none>           <none>
kube-system   kube-proxy-djvp4                           1/1     Running   0              105m   172.31.24.77    ip-172-31-24-77    <none>           <none>
kube-system   kube-proxy-f2f57                           1/1     Running   0              105m   172.31.19.147   ip-172-31-19-147   <none>           <none>
kube-system   kube-proxy-tshff                           1/1     Running   0              105m   172.31.25.32    ip-172-31-25-32    <none>           <none>
kube-system   kube-scheduler-ip-172-31-19-147            1/1     Running   0              105m   172.31.19.147   ip-172-31-19-147   <none>           <none>
kube-system   weave-net-4qfbb                            2/2     Running   1 (103m ago)   103m   172.31.19.147   ip-172-31-19-147   <none>           <none>
kube-system   weave-net-htlrp                            2/2     Running   1 (103m ago)   103m   172.31.25.32    ip-172-31-25-32    <none>           <none>
kube-system   weave-net-nltmv                            2/2     Running   1 (103m ago)   103m   172.31.24.77    ip-172-31-24-77    <none>           <none>
```

##### Executando nosso primeiro pod no k8s

Iremos iniciar o nosso primeiro *pod* no k8s. Para isso, executaremos o comando a seguir.

```
kubectl run nginx --image nginx

pod/nginx created
```

Listando os *pods* com ``kubectl get pods``, obteremos a seguinte saÃ­da.

```
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          66s
```

Vamos olhar agora a descriÃ§Ã£o desse objeto dentro do *cluster*.

```
kubectl describe pod nginx

Name:         nginx
Namespace:    default
Priority:     0
Node:         ip-172-31-25-32/172.31.25.32
Start Time:   Sun, 07 Aug 2022 08:53:24 +0000
Labels:       run=nginx
Annotations:  <none>
Status:       Running
IP:           10.40.0.1
IPs:
  IP:  10.40.0.1
Containers:
  nginx:
    Container ID:   containerd://d7ae9933e65477eed7ff04a107fb3a3adb6a634bc713282421bbdf0e1c30bf7b
    Image:          nginx
    Image ID:       docker.io/library/nginx@sha256:ecc068890de55a75f1a32cc8063e79f90f0b043d70c5fcf28f1713395a4b3d49
    Port:           <none>
    Host Port:      <none>
    State:          Running
      Started:      Sun, 07 Aug 2022 08:53:30 +0000
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-tmjgq (ro)
Conditions:
  Type              Status
  Initialized       True 
  Ready             True 
  ContainersReady   True 
  PodScheduled      True 
Volumes:
  kube-api-access-tmjgq:
    Type:                    Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds:  3607
    ConfigMapName:           kube-root-ca.crt
    ConfigMapOptional:       <nil>
    DownwardAPI:             true
QoS Class:                   BestEffort
Node-Selectors:              <none>
Tolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age   From               Message
  ----    ------     ----  ----               -------
  Normal  Scheduled  16s   default-scheduler  Successfully assigned default/nginx to ip-172-31-25-32
  Normal  Pulling    16s   kubelet            Pulling image "nginx"
  Normal  Pulled     10s   kubelet            Successfully pulled image "nginx" in 5.387864178s
  Normal  Created    10s   kubelet            Created container nginx
  Normal  Started    10s   kubelet            Started container nginx

```

##### Verificar os Ãºltimos eventos do cluster

VocÃª pode verificar quais sÃ£o os Ãºltimos eventos do *cluster* com o comando ``kubectl get events``. SerÃ£o mostrados eventos como: o *download* de imagens do Docker Hub (ou de outro *registry* configurado), a criaÃ§Ã£o/remoÃ§Ã£o de *pods*, etc.

A saÃ­da a seguir mostra o resultado da criaÃ§Ã£o do nosso contÃªiner com Nginx.

```
kubectl get events

LAST SEEN   TYPE     REASON      OBJECT      MESSAGE
44s         Normal   Scheduled   pod/nginx   Successfully assigned default/nginx to ip-172-31-25-32
44s         Normal   Pulling     pod/nginx   Pulling image "nginx"
38s         Normal   Pulled      pod/nginx   Successfully pulled image "nginx" in 5.387864178s
38s         Normal   Created     pod/nginx   Created container nginx
38s         Normal   Started     pod/nginx   Started container nginx

```

No resultado do comando anterior Ã© possÃ­vel observar que a execuÃ§Ã£o do nginx ocorreu no *namespace* default e que a imagem **nginx** nÃ£o existia no repositÃ³rio local e, sendo assim, teve de ser feito download da imagem.


##### Efetuar o dump de um objeto em formato YAML

Assim como quando se estÃ¡ trabalhando com *stacks* no Docker Swarm, normalmente recursos no k8s sÃ£o declarados em arquivos **YAML** ou **JSON** e depois manipulados atravÃ©s do ``kubectl``.

Para nos poupar o trabalho de escrever o arquivo inteiro, pode-se utilizar como *template* o *dump* de um objeto jÃ¡ existente no k8s, como mostrado a seguir.

```
kubectl get pod nginx -o yaml > meu-primeiro.yaml
```

SerÃ¡ criado um novo arquivo chamado ``meu-primeiro.yaml``, resultante do redirecionamento da saÃ­da do comando ``kubectl get pod nginx -o yaml``.

Abrindo o arquivo com ``vim meu-primeiro.yaml`` (vocÃª pode utilizar o editor que vocÃª preferir), teremos o seguinte conteÃºdo.

```yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: "2022-08-07T08:53:24Z"
  labels:
    run: nginx
  name: nginx
  namespace: default
  resourceVersion: "9598"
  uid: d0928186-bf6d-459b-aca6-9b0d84b40e9c
spec:
  containers:
  - image: nginx
    imagePullPolicy: Always
    name: nginx
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-tmjgq
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: ip-172-31-25-32
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: {}
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - name: kube-api-access-tmjgq
    projected:
      defaultMode: 420
      sources:
      - serviceAccountToken:
          expirationSeconds: 3607
          path: token
      - configMap:
          items:
          - key: ca.crt
            path: ca.crt
          name: kube-root-ca.crt
      - downwardAPI:
          items:
          - fieldRef:
              apiVersion: v1
              fieldPath: metadata.namespace
            path: namespace
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2022-08-07T08:53:24Z"
    status: "True"
    type: Initialized
  - lastProbeTime: null
    lastTransitionTime: "2022-08-07T08:53:30Z"
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2022-08-07T08:53:30Z"
    status: "True"
    type: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: "2022-08-07T08:53:24Z"
    status: "True"
    type: PodScheduled
  containerStatuses:
  - containerID: containerd://d7ae9933e65477eed7ff04a107fb3a3adb6a634bc713282421bbdf0e1c30bf7b
    image: docker.io/library/nginx:latest
    imageID: docker.io/library/nginx@sha256:ecc068890de55a75f1a32cc8063e79f90f0b043d70c5fcf28f1713395a4b3d49
    lastState: {}
    name: nginx
    ready: true
    restartCount: 0
    started: true
    state:
      running:
        startedAt: "2022-08-07T08:53:30Z"
  hostIP: 172.31.25.32
  phase: Running
  podIP: 10.40.0.1
  podIPs:
  - ip: 10.40.0.1
  qosClass: BestEffort
  startTime: "2022-08-07T08:53:24Z"
```

Observando o arquivo anterior, notamos que este reflete o **estado** do *pod*. NÃ³s desejamos utilizar tal arquivo apenas como um modelo, e sendo assim, podemos apagar as entradas que armazenam dados de estado desse *pod*, como *status* e todas as demais configuraÃ§Ãµes que sÃ£o especÃ­ficas dele. O arquivo final ficarÃ¡ com o conteÃºdo semelhante a este:

```yaml
  apiVersion: v1
  kind: Pod
  metadata:
    labels:
      run: nginx
    name: nginx
  spec:
    containers:
    - image: nginx
      name: nginx
      resources: {}
    dnsPolicy: ClusterFirst
    restartPolicy: Always
  status: {}
```

Vamos agora remover o nosso *pod* com o seguinte comando.

```
kubectl delete pod nginx
```

A saÃ­da deve ser algo como:

```
pod "nginx" deleted
```

Vamos recriÃ¡-lo, agora a partir do nosso arquivo YAML.

```
kubectl create -f meu-primeiro.yaml

pod/nginx created
```

Observe que nÃ£o foi necessÃ¡rio informar ao ``kubectl`` qual tipo de recurso seria criado, pois isso jÃ¡ estÃ¡ contido dentro do arquivo.

Listando os *pods* disponÃ­veis com o seguinte comando.

```
kubectl get pods
```

Deve-se obter uma saÃ­da similar Ã  esta:

```
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          109s
```

Uma outra forma de criar um arquivo de *template* Ã© atravÃ©s da opÃ§Ã£o ``--dry-run`` do ``kubectl``, com o funcionamento ligeiramente diferente dependendo do tipo de recurso que serÃ¡ criado. Exemplos:

Para a criaÃ§Ã£o do template de um *pod*:

```
kubectl run meu-nginx --image nginx --dry-run=client -o yaml > pod-template.yaml
```

Para a criaÃ§Ã£o do *template* de um *deployment*:

```
kubectl create deployment meu-nginx --image=nginx --dry-run=client -o yaml > deployment-template.yaml
```

A vantagem deste mÃ©todo Ã© que nÃ£o hÃ¡ a necessidade de limpar o arquivo, alÃ©m de serem apresentadas apenas as opÃ§Ãµes necessÃ¡rias do recurso.

#### Socorro, sÃ£o muitas opÃ§Ãµes!

Calma, nÃ³s sabemos. Mas o ``kubectl`` pode lhe auxiliar um pouco em relaÃ§Ã£o a isso. Ele contÃ©m a opÃ§Ã£o ``explain``, que vocÃª pode utilizar caso precise de ajuda com alguma opÃ§Ã£o em especÃ­fico dos arquivos de recurso. A seguir alguns exemplos de sintaxe.

```
kubectl explain [recurso]

kubectl explain [recurso.caminho.para.spec]

kubectl explain [recurso.caminho.para.spec] --recursive
```

Exemplos:

```
kubectl explain deployment

kubectl explain pod --recursive

kubectl explain deployment.spec.template.spec
```

#### Expondo o pod e criando um Service

Dispositivos fora do *cluster*, por padrÃ£o, nÃ£o conseguem acessar os *pods* criados, como Ã© comum em outros sistemas de contÃªineres. Para expor um *pod*, execute o comando a seguir.

```
kubectl expose pod nginx
```

SerÃ¡ apresentada a seguinte mensagem de erro:

```
error: couldn't find port via --port flag or introspection
See 'kubectl expose -h' for help and examples
```

O erro ocorre devido ao fato do k8s nÃ£o saber qual Ã© a porta de destino do contÃªiner que deve ser exposta (no caso, a 80/TCP). Para configurÃ¡-la, vamos primeiramente remover o nosso *pod* antigo:

```
kubectl delete -f meu-primeiro.yaml
```

Abra agora o arquivo ``meu-primeiro.yaml`` e adicione o bloco a seguir.

```yaml
...
spec:
       containers:
       - image: nginx
         imagePullPolicy: Always
         ports:
         - containerPort: 80
         name: nginx
         resources: {}
...
```

> **AtenÃ§Ã£o!!!** Arquivos YAML utilizam para sua tabulaÃ§Ã£o dois espaÃ§os e nÃ£o *tab*.

Feita a modificaÃ§Ã£o no arquivo, salve-o e crie novamente o *pod* com o comando a seguir.

```
kubectl create -f meu-primeiro.yaml

pod/nginx created
```

Liste o pod.

```
kubectl get pod nginx

NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          32s
```

O comando a seguir cria um objeto do k8s chamado de *Service*, que Ã© utilizado justamente para expor *pods* para acesso externo.

```
kubectl expose pod nginx
```

Podemos listar todos os *services* com o comando a seguir.

```
kubectl get services

NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
kubernetes   ClusterIP   10.96.0.1       <none>        443/TCP   8d
nginx        ClusterIP   10.105.41.192   <none>        80/TCP    2m30s
```

Como Ã© possÃ­vel observar, hÃ¡ dois *services* no nosso *cluster*: o primeiro Ã© para uso do prÃ³prio k8s, enquanto o segundo foi o quÃª acabamos de criar. Utilizando o ``curl`` contra o endereÃ§o IP mostrado na coluna *CLUSTER-IP*, deve nos ser apresentada a tela principal do Nginx.

```
curl 10.105.41.192

<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
    body {
        width: 35em;
        margin: 0 auto;
        font-family: Tahoma, Verdana, Arial, sans-serif;
    }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.</p>

<p>For online documentation and support please refer to
<a href="http://nginx.org/">nginx.org</a>.<br/>
Commercial support is available at
<a href="http://nginx.com/">nginx.com</a>.</p>

<p><em>Thank you for using nginx.</em></p>
</body>
</html>
```

Este *pod* estÃ¡ disponÃ­vel para acesso a partir de qualquer nÃ³ do *cluster*.


#### Limpando tudo e indo para casa

Para mostrar todos os recursos recÃ©m criados, pode-se utilizar uma das seguintes opÃ§Ãµes a seguir.

```
kubectl get all

kubectl get pod,service

kubectl get pod,svc
```

Note que o k8s nos disponibiliza algumas abreviaÃ§Ãµes de seus recursos. Com o tempo vocÃª irÃ¡ se familiar com elas. Para apagar os recursos criados, vocÃª pode executar os seguintes comandos.

```
kubectl delete -f meu-primeiro.yaml

kubectl delete service nginx
```

Liste novamente os recursos para ver se os mesmos ainda estÃ£o presentes.
